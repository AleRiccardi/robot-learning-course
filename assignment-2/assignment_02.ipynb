{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arms rewards\n",
    "INTERVALS = [(-4,3),(1,5),(2,3),(-2,5),(0,4),(1,4),(3,7)]\n",
    "\n",
    "# Expected value of every arm\n",
    "arms_expected = np.zeros(7)\n",
    "for x, interval in enumerate(INTERVALS):\n",
    "    arms_expected[x] = (interval[-1] + interval[0])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected reward for each arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5,  3. ,  2.5,  1.5,  2. ,  2.5,  5. ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arms_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected reward random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2857142857142856"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arms_expected.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sample from reward interval\n",
    "def get_reward(action):\n",
    "    interval = INTERVALS[action]\n",
    "    return np.random.randint(interval[0], interval[1])\n",
    "\n",
    "# Get the greedy action\n",
    "def get_greedy_action(Q):\n",
    "    avg_rews = [sum(arm_rews) / len(arm_rews) if len(arm_rews) > 0 else -np.inf for arm_rews in Q]\n",
    "    action = avg_rews.index(max(avg_rews))\n",
    "    return action\n",
    "\n",
    "def compute_action(Q, greedy=True):\n",
    "    if greedy:\n",
    "        action = get_greedy_action(Q)\n",
    "    else:\n",
    "        action = np.random.randint(0,7)\n",
    "    \n",
    "    # Sample new reward for\n",
    "    # the given action\n",
    "    reward = get_reward(action)\n",
    "\n",
    "    # Update Q table\n",
    "    Q[action].append(reward)\n",
    "\n",
    "XYs = [[[], []] for x in range(8)]\n",
    "\n",
    "def save_usage(Q, count):\n",
    "\n",
    "    percentages = [0 for x in range(7)]\n",
    "    \n",
    "    for idx in range(len(Q)):\n",
    "        if len(Q[idx]) != 0:\n",
    "            percentages[idx] = len(Q[idx]) / len(np.concatenate(Q).ravel())\n",
    "\n",
    "    # Plot percentage arms\n",
    "    for idx in range(len(Q)):\n",
    "        XYs[idx][0].append(count)\n",
    "        XYs[idx][1].append(percentages[idx])\n",
    "    \n",
    "    XYs[7][0].append(count)\n",
    "    XYs[7][1].append(np.concatenate(Q).ravel().mean())    \n",
    "\n",
    "def show_usage():\n",
    "    for idx, color in enumerate(colors):\n",
    "        # Plot avarage result\n",
    "        plt.plot(XYs[idx][0], XYs[idx][1], color)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-7-50d94f9e037c>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-50d94f9e037c>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    colors = [\\\"-r\\\", \\\"-b\\\", \\\"-c\\\", \\\"-g\\\",\\\"-m\\\", \\\"-y\\\", \\\"-y\\\", \\\"-k\\\"]\u001b[0m\n\u001b[0m                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# Action/Reward table\n",
    "Q = [[0] for x in range(7)]\n",
    "\n",
    "# Epsilon\n",
    "e = 0.1\n",
    "\n",
    "# Colors\n",
    "colors = [\"-r\", \"-b\", \"-c\", \"-g\",\"-m\", \"-y\", \"-y\", \"-k\"]\n",
    "\n",
    "for x in range(2000):\n",
    "    # Exploit or Explore\n",
    "    if np.random.uniform(0,1) > 0.1:\n",
    "        compute_action(Q, True)\n",
    "    else: \n",
    "        # Choose random action (i.e. arm)\n",
    "        compute_action(Q, False)\n",
    "    \n",
    "    if x % 100 == 0:\n",
    "        save_usage(Q, x)\n",
    "\n",
    "show_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to formulate the example as a **MDP** the **Markov property** has to hold (i.e. we have conditional indepence on the previous information, given the current state and action). We thereby identifiy the features of the **state**, the possible **actions** and some examples of the **transitional probabilities** and **reward expectations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State\n",
    "\n",
    "The state needs to have the following features to it:\n",
    "\n",
    "- Current task\n",
    "- Whether a task was reattempted before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "\n",
    "The possible actions at a current state are:\n",
    "\n",
    "- Take an exam\n",
    "- Retake an exam\n",
    "- Skip an exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transitional Probabilities\n",
    "\n",
    "We give below the example for state 1 (where 1 denotes the first exam). In particular we show what are the possible transitions we can have, given the actions identified above, and the state features defined.\n",
    "\n",
    "- **P11 = Pr{s'=1 | s=1, a=\"take\"} = 0.85**\n",
    "- **P12 = Pr{s'=2 | s=1, a=\"retake\"} = 0.85**\n",
    "- **P12 = Pr{s'=2 | s=1, a=\"take\"} = 0.15**\n",
    "- **P12 = Pr{s'=2 | s=1, a=\"skip\"} = 1**\n",
    "\n",
    "and the same holds for the remaining states and transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewards Expectations\n",
    "\n",
    "Similarly to the above, we show the identified rewards expectations for state 1 (where 1 is the first exam).\n",
    "\n",
    "- **R11 = E{s'=1 | s=1, a=\"take\"} = 0**\n",
    "- **R12 = E{s'=2 | s=1, a=\"take\"} = 8**\n",
    "- **R12 = E{s'=2 | s=1, a=\"skip\"} = 0**\n",
    "- **R12 = E{s'=2 | s=1, a=\"retake\"} = 0**\n",
    "- **R12 = E{s'=2 | s=1, a=\"retake\"} = 8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy A:  38\n",
      "Policy B:  0\n"
     ]
    }
   ],
   "source": [
    "# Environment Data\n",
    "data = [(8, 0.15),(6, 0.4),(10, 0.25),(2, 0.6),(7, 0.35),(3, 0.5),(20, 0.2)]\n",
    "\n",
    "def policy(env):\n",
    "    # Define state\n",
    "    state = {\"current\": 0,\n",
    "             \"reattempted\": False}\n",
    "    \n",
    "    # Policy reward\n",
    "    reward = 0\n",
    "    \n",
    "    # Go through the exams\n",
    "    while state[\"current\"] < 7:\n",
    "        # Current exam\n",
    "        current = state[\"current\"]\n",
    "\n",
    "        # Action decision\n",
    "        if np.random.uniform(0,1) <= env[current][1]:\n",
    "            # Action: Take\n",
    "            reward += env[current][0]\n",
    "        elif not state[\"reattempted\"] and np.random.uniform(0,1) <= env[current][1]:\n",
    "            # Action: Retake\n",
    "            reward += env[current][0]\n",
    "        else:\n",
    "            # Action: Skip\n",
    "            state[\"reattempted\"] = True\n",
    "\n",
    "        # Update state\n",
    "        state[\"current\"] += 1\n",
    "    \n",
    "    return reward\n",
    "\n",
    "print(\"Policy A: \", policy(data))\n",
    "print(\"Policy B: \", policy(sorted(data, key=lambda x: x[1], reverse=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.5\n",
    "A Markov assumption is not justified when it is required to keep track of a specific information regarding all the previous states. For example, if we consider an automated (robot) delivery truck that needs to ship to every costumer a specific parcel and report at the end whether each parcel was delivered or not. In this case, it is fundamental to keep track if every costumer received its parcel or not and hence to include at every state the delivery status of every costumer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

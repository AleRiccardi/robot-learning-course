{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0 -1.0 -1.0 -25.0 -25.0 -25.0 -25.0]\n",
      " [-1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -25.0]\n",
      " [-1.0 -1.0 -25.0 -1.0 -25.0 -1.0 100.0]\n",
      " [-1.0 -25.0 -25.0 -1.0 -1.0 -25.0 -1.0]\n",
      " [-1.0 -1.0 -1.0 -25.0 -25.0 -25.0 -1.0]]\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)}, threshold=sys.maxsize)\n",
    "\n",
    "# Global variables\n",
    "EPS = 0.1\n",
    "ALPHA = 0.9\n",
    "GAMMA = 0.8\n",
    "GOAL = (2,6)\n",
    "START = (2,0)\n",
    "WORLD = -1. * np.ones((5,7))\n",
    "OBSTACLES = [(0,3),(0,4),(0,5),(0,6),(1,6),(2,2),(2,4),(3,1),(3,2),(3,5),(4,3),(4,4),(4,5)]\n",
    "ACTIONS = {\n",
    "    'ur': [('ur', 0.8), ('u', 0.1), ('r', 0.1)],\n",
    "    'r': [('r', 0.8), ('ur', 0.1), ('dr', 0.1)], \n",
    "    'dr': [('dr', 0.8), ('r', 0.1), ('d', 0.1)]\n",
    "}\n",
    "\n",
    "# Populate world\n",
    "WORLD[GOAL] = 100\n",
    "\n",
    "for obstacle in OBSTACLES:\n",
    "    WORLD[obstacle] = -25\n",
    "\n",
    "print(WORLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move\n",
    "def move(state, action):\n",
    "    new_state = state.copy()\n",
    "    \n",
    "    # Right\n",
    "    if action == \"r\":\n",
    "        new_state[\"x\"] += 1\n",
    "    # Lower right\n",
    "    elif action == \"dr\":\n",
    "        new_state[\"x\"] += 1\n",
    "        new_state[\"y\"] += 1\n",
    "    # Down\n",
    "    elif action == \"d\":\n",
    "        new_state[\"y\"] += 1\n",
    "    # Lower left\n",
    "    elif action == \"dl\":\n",
    "        new_state[\"x\"] -= 1\n",
    "        new_state[\"y\"] += 1\n",
    "    # Left\n",
    "    elif action == \"l\":\n",
    "        new_state[\"x\"] -= 1\n",
    "    # Upper left\n",
    "    elif action == \"ul\":\n",
    "        new_state[\"x\"] -= 1\n",
    "        new_state[\"y\"] -= 1\n",
    "    # Up\n",
    "    elif action == \"u\":\n",
    "        new_state[\"y\"] -= 1\n",
    "    # Upper right\n",
    "    if action == \"ur\":\n",
    "        new_state[\"x\"] += 1\n",
    "        new_state[\"y\"] -= 1\n",
    "\n",
    "    return new_state\n",
    "\n",
    "# Check if state out of bound\n",
    "# and returns truncated state\n",
    "def truncate(state):\n",
    "    # Truncated state\n",
    "    new_state = state.copy()\n",
    "    \n",
    "    # Check if out of bounds\n",
    "    if new_state['y'] < 0:\n",
    "        new_state['y'] += 1\n",
    "        \n",
    "    if new_state['y'] >= WORLD.shape[0]:\n",
    "        new_state['y'] -= 1\n",
    "            \n",
    "    if new_state['x'] < 0:\n",
    "        new_state['x'] += 1\n",
    "        \n",
    "    if new_state['x'] >= WORLD.shape[1]:\n",
    "        new_state['x'] -= 1\n",
    "    \n",
    "    return new_state\n",
    "\n",
    "# Check if state terminal\n",
    "def terminal(state):\n",
    "    return True if (state['y'], state['x']) == GOAL or \\\n",
    "                   (state['y'], state['x']) in OBSTACLES \\\n",
    "                else False\n",
    "\n",
    "# Step function\n",
    "def step(state, action):\n",
    "    # Returns\n",
    "    done = None\n",
    "    reward = None\n",
    "    \n",
    "    # Move based on action\n",
    "    new_state = move(state, action)\n",
    "    \n",
    "    # Check that state is valid\n",
    "    new_state = truncate(new_state)\n",
    "    \n",
    "    # Check if state is terminal\n",
    "    done = terminal(new_state)\n",
    "    \n",
    "    # Get reward for new state\n",
    "    reward = WORLD[new_state['y'], new_state['x']]\n",
    "    \n",
    "    return (new_state, reward, done)\n",
    "\n",
    "def generate_action():\n",
    "    action = None\n",
    "    prob = np.random.uniform(0.0, 1.0)\n",
    "    \n",
    "    # Apply policy\n",
    "    if prob < 0.5:\n",
    "        action = 'ur' if prob > 0.25 else 'dr'\n",
    "    else:\n",
    "        action = 'r'\n",
    "        \n",
    "    # Apply non-det. action\n",
    "    prob = np.random.uniform(0.0, 1.0)\n",
    "    if prob < ACTIONS[action][0][1]:\n",
    "        action = ACTIONS[action][0][0]\n",
    "    else:\n",
    "        action = (ACTIONS[action][1][0] \n",
    "                  if prob > ACTIONS[action][1][1] \n",
    "                  else ACTIONS[action][2][0])\n",
    "    return action\n",
    "\n",
    "\n",
    "def TD_eval(loops=1000):\n",
    "    V = np.zeros_like(WORLD)\n",
    "\n",
    "    for count in range(loops): \n",
    "        s = {'y': START[0], 'x': START[1]}\n",
    "\n",
    "        # Episode loop\n",
    "        while True:\n",
    "            action = generate_action()\n",
    "            n_s, reward, done = step(s, action)\n",
    "                        \n",
    "            V[s['y'], s['x']] = (V[s['y'], s['x']] + ALPHA * \n",
    "                                (reward + GAMMA*V[n_s['y'], n_s['x']]-V[s['y'], s['x']]))\n",
    "            s = n_s\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.8, -20.8, -25.0, 0.0, 0.0, 0.0, 0.0],\n",
       "       [-15.2, -21.0, -24.3, -24.8, 68.2, -13.7, 0.0],\n",
       "       [-18.7, -24.8, 0.0, -19.3, 0.0, -12.8, 0.0],\n",
       "       [0.0, 0.0, 0.0, 0.0, -17.8, 0.0, 96.4],\n",
       "       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = TD_eval()\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_arrow(action):\n",
    "    if action == 'r':\n",
    "        return u'\\u2192'\n",
    "    elif action == 'l':\n",
    "        return u'\\u2190'\n",
    "    elif action == 'u':\n",
    "        return u'\\u2191'\n",
    "    elif action == 'd':\n",
    "        return u'\\u2193'\n",
    "    elif action == 'ul':\n",
    "        return u'\\u2196'\n",
    "    elif action == 'ur':\n",
    "        return u'\\u2197'\n",
    "    elif action == 'dr':\n",
    "        return u'\\u2198'\n",
    "    else:\n",
    "        return u'\\u2199'\n",
    "\n",
    "def eps_greedy(Q, s):\n",
    "    # Action from policy\n",
    "    action = None\n",
    "    \n",
    "    # Encode state\n",
    "    key_s = ','.join(str(x) for x in s.values())\n",
    "        \n",
    "    if key_s in Q.keys() and Q[key_s]:\n",
    "        if np.random.uniform(0.0, 1.0) > EPS: \n",
    "            action = max(Q[key_s].items(), key=operator.itemgetter(1))[0]\n",
    "        else:\n",
    "            action = generate_action()\n",
    "    else:\n",
    "        action = generate_action()\n",
    "    \n",
    "    # Create entry if not existent\n",
    "    if key_s not in Q.keys():\n",
    "        Q[key_s] = {}\n",
    "    if action not in Q[key_s].keys():\n",
    "        Q[key_s][action] = 0\n",
    "    \n",
    "    return action\n",
    "\n",
    "def max_action(Q, s, value=True):\n",
    "    # Action from policy\n",
    "    action = None\n",
    "    \n",
    "    # Encode state\n",
    "    key_s = ','.join(str(x) for x in s.values())\n",
    "        \n",
    "    if key_s in Q.keys() and Q[key_s]:\n",
    "        # Get max action\n",
    "        action = max(Q[key_s].items(), key=operator.itemgetter(1))[0]\n",
    "        \n",
    "        if value:\n",
    "            return Q[key_s][action]\n",
    "        else:\n",
    "            return action\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def show_policy(Q):\n",
    "    # Policy\n",
    "    P = WORLD.copy().astype(object)\n",
    "    \n",
    "    # Loop over all states\n",
    "    for key in Q.keys():\n",
    "        # Get state representations\n",
    "        state_list = key.split(\",\")\n",
    "        state = (int(state_list[0]), int(state_list[1]))\n",
    "        state_dict = {'y': int(state_list[0]), 'x': int(state_list[1])}\n",
    "        \n",
    "        # Get max action for state\n",
    "        action = max_action(Q, state_dict, value=False)\n",
    "        \n",
    "        # Insert policy action\n",
    "        P[state] = which_arrow(action)\n",
    "    \n",
    "    return P\n",
    "\n",
    "# Q-Learning\n",
    "def q_learning(loops=10000):\n",
    "    # Action value function\n",
    "    Q = {}\n",
    "    \n",
    "    for count in range(loops):\n",
    "        # Initial state\n",
    "        s = {'y': START[0], 'x': START[1]}\n",
    "\n",
    "        # Episode loop\n",
    "        while True:\n",
    "            action = None\n",
    "            \n",
    "            # EPS greedy\n",
    "            action = eps_greedy(Q, s)\n",
    "\n",
    "            # Get next state, reward and if terminal    \n",
    "            n_s, r, done = step(s, action)\n",
    "            \n",
    "            # Encode state\n",
    "            key_s = ','.join(str(x) for x in s.values())\n",
    "            \n",
    "            # Update Q\n",
    "            Q[key_s][action] = (Q[key_s][action] +\n",
    "                              ALPHA * (r + GAMMA * max_action(Q, n_s) - Q[key_s][action]))\n",
    "            \n",
    "            # Update state\n",
    "            s = n_s\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning policy: \n",
      "\n",
      " [[-1.0 '↘' '↘' -25.0 -25.0 -25.0 -25.0]\n",
      " ['↗' '→' '↘' '→' '↘' '↘' -25.0]\n",
      " ['↗' '↗' -25.0 '↘' -25.0 '→' 100.0]\n",
      " [-1.0 -25.0 -25.0 -1.0 '↗' -25.0 '↗']\n",
      " [-1.0 -1.0 -1.0 -25.0 -25.0 -25.0 '↗']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q-Learning policy: \\n\\n\", show_policy(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

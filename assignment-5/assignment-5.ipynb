{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0 -1.0 -1.0 -25.0 -25.0 -25.0 -25.0]\n",
      " [-1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -25.0]\n",
      " [-1.0 -1.0 -25.0 -1.0 -25.0 -1.0 100.0]\n",
      " [-1.0 -25.0 -25.0 -1.0 -1.0 -25.0 -1.0]\n",
      " [-1.0 -1.0 -1.0 -25.0 -25.0 -25.0 -1.0]]\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)}, threshold=sys.maxsize)\n",
    "\n",
    "# Global variables\n",
    "EPS = 0.1\n",
    "GOAL = (2,6)\n",
    "START = (2,0)\n",
    "WORLD = -1. * np.ones((5,7))\n",
    "OBSTACLES = [(0,3),(0,4),(0,5),(0,6),(1,6),(2,2),(2,4),(3,1),(3,2),(3,5),(4,3),(4,4),(4,5)]\n",
    "    \n",
    "# Populate world\n",
    "WORLD[GOAL] = 100\n",
    "\n",
    "for obstacle in OBSTACLES:\n",
    "    WORLD[obstacle] = -25\n",
    "\n",
    "print(WORLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.9\n",
    "GAMMA = 0.8\n",
    "\n",
    "# Move\n",
    "def move(state, action):\n",
    "    new_state = state.copy()\n",
    "    \n",
    "    # Right\n",
    "    if action == \"r\":\n",
    "        new_state[\"x\"] += 1\n",
    "    # Lower right\n",
    "    elif action == \"dr\":\n",
    "        new_state[\"x\"] += 1\n",
    "        new_state[\"y\"] += 1\n",
    "    # Down\n",
    "    elif action == \"d\":\n",
    "        new_state[\"y\"] += 1\n",
    "    # Lower left\n",
    "    elif action == \"dl\":\n",
    "        new_state[\"x\"] -= 1\n",
    "        new_state[\"y\"] += 1\n",
    "    # Left\n",
    "    elif action == \"l\":\n",
    "        new_state[\"x\"] -= 1\n",
    "    # Upper left\n",
    "    elif action == \"ul\":\n",
    "        new_state[\"x\"] -= 1\n",
    "        new_state[\"y\"] -= 1\n",
    "    # Up\n",
    "    elif action == \"u\":\n",
    "        new_state[\"y\"] -= 1\n",
    "    # Upper right\n",
    "    if action == \"ur\":\n",
    "        new_state[\"x\"] += 1\n",
    "        new_state[\"y\"] -= 1\n",
    "\n",
    "    return new_state\n",
    "\n",
    "# Check if state out of bound\n",
    "# and returns truncated state\n",
    "def truncate(state):\n",
    "    # Truncated state\n",
    "    new_state = state.copy()\n",
    "    \n",
    "    # Check if out of bounds\n",
    "    if new_state['y'] < 0:\n",
    "        new_state['y'] += 1\n",
    "        \n",
    "    if new_state['y'] >= WORLD.shape[0]:\n",
    "        new_state['y'] -= 1\n",
    "            \n",
    "    if new_state['x'] < 0:\n",
    "        new_state['x'] += 1\n",
    "        \n",
    "    if new_state['x'] >= WORLD.shape[1]:\n",
    "        new_state['x'] -= 1\n",
    "    \n",
    "    return new_state\n",
    "\n",
    "# Check if state terminal\n",
    "def terminal(state):\n",
    "    return True if (state['y'], state['x']) == GOAL or \\\n",
    "                   (state['y'], state['x']) in OBSTACLES \\\n",
    "                else False\n",
    "\n",
    "# Step function\n",
    "def step(state, action):\n",
    "    # Returns\n",
    "    done = None\n",
    "    reward = None\n",
    "    \n",
    "    # Move based on action\n",
    "    new_state = move(state, action)\n",
    "    \n",
    "    # Check that state is valid\n",
    "    new_state = truncate(new_state)\n",
    "    \n",
    "    # Check if state is terminal\n",
    "    done = terminal(new_state)\n",
    "    \n",
    "    # Get reward for new state\n",
    "    reward = WORLD[new_state['y'], new_state['x']]\n",
    "    \n",
    "    return (new_state, reward, done)\n",
    "\n",
    "\n",
    "ACTIONS = {\n",
    "    'ur': [('ur', 0.8), ('u', 0.1), ('r', 0.1)],\n",
    "    'r': [('r', 0.8), ('ur', 0.1), ('dr', 0.1)], \n",
    "    'dr': [('dr', 0.8), ('r', 0.1), ('d', 0.1)]\n",
    "}\n",
    "\n",
    "def generate_action():\n",
    "    action = None\n",
    "    prob = np.random.uniform(0.0, 1.0)\n",
    "    \n",
    "    # Apply policy\n",
    "    if prob < 0.5:\n",
    "        action = 'ur' if prob > 0.25 else 'dr'\n",
    "    else:\n",
    "        action = 'r'\n",
    "        \n",
    "    # Apply non-det. action\n",
    "    prob = np.random.uniform(0.0, 1.0)\n",
    "    if prob < ACTIONS[action][0][1]:\n",
    "        action = ACTIONS[action][0][0]\n",
    "    else:\n",
    "        action = (ACTIONS[action][1][0] \n",
    "                  if prob > ACTIONS[action][1][1] \n",
    "                  else ACTIONS[action][2][0])\n",
    "    return action\n",
    "\n",
    "\n",
    "def TD_eval(loops=1000):\n",
    "    V = np.zeros_like(WORLD)\n",
    "\n",
    "    for count in range(loops): \n",
    "        s = {'y': START[0], 'x': START[1]}\n",
    "\n",
    "        # Episode loop\n",
    "        while True:\n",
    "            action = generate_action()\n",
    "            n_s, reward, done = step(s, action)\n",
    "                        \n",
    "            V[s['y'], s['x']] = (V[s['y'], s['x']] + ALPHA * \n",
    "                                (reward + GAMMA*V[n_s['y'], n_s['x']]-V[s['y'], s['x']]))\n",
    "            s = n_s\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-15.8, -20.7, -24.6, 0.0, 0.0, 0.0, 0.0],\n",
       "       [-17.2, -20.4, -13.9, -11.8, -12.0, -23.9, 0.0],\n",
       "       [-21.0, -25.0, 0.0, -20.8, 0.0, 67.8, 0.0],\n",
       "       [0.0, 0.0, 0.0, 0.0, -15.6, 0.0, 99.7],\n",
       "       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = TD_eval()\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(Q, s):\n",
    "    # Action from policy\n",
    "    action = None\n",
    "    \n",
    "    # Encode state\n",
    "    key_s = ','.join(str(x) for x in s.values())\n",
    "        \n",
    "    if key_s in Q.keys() and Q[key_s]:\n",
    "        if np.random.uniform(0.0, 1.0) > EPS: \n",
    "            action = max(Q[key_s].items(), key=operator.itemgetter(1))[0]\n",
    "        else:\n",
    "            action = generate_action()\n",
    "    else:\n",
    "        action = generate_action()\n",
    "    \n",
    "    # Create entry if not existent\n",
    "    if key_s not in Q.keys():\n",
    "        Q[key_s] = {}\n",
    "    if action not in Q[key_s].keys():\n",
    "        Q[key_s][action] = 0\n",
    "    \n",
    "    return action\n",
    "\n",
    "def max_action(Q, s):\n",
    "    # Action from policy\n",
    "    action = None\n",
    "    \n",
    "    # Encode state\n",
    "    key_s = ','.join(str(x) for x in s.values())\n",
    "        \n",
    "    if key_s in Q.keys() and Q[key_s]:\n",
    "        action = max(Q[key_s].items(), key=operator.itemgetter(1))[0]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    return Q[key_s][action]\n",
    "\n",
    "# Q-Learning\n",
    "def q_learning(loops=10000):\n",
    "    # Action value function\n",
    "    Q = {}\n",
    "    \n",
    "    for count in range(loops):\n",
    "        # Initial state\n",
    "        s = {'y': START[0], 'x': START[1]}\n",
    "\n",
    "        # Episode loop\n",
    "        while True:\n",
    "            action = None\n",
    "            \n",
    "            # EPS greedy\n",
    "            action = eps_greedy(Q, s)\n",
    "\n",
    "            # Get next state, reward and if terminal    \n",
    "            n_s, r, done = step(s, action)\n",
    "            \n",
    "            # Encode state\n",
    "            key_s = ','.join(str(x) for x in s.values())\n",
    "            \n",
    "            # Update Q\n",
    "            Q[key_s][action] = (Q[key_s][action] +\n",
    "                              ALPHA * (r + GAMMA * max_action(Q, n_s) - Q[key_s][action]))\n",
    "            \n",
    "            # Update state\n",
    "            s = n_s\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2,0': {'ur': 29.40640000000001,\n",
       "  'r': 29.40640000000001,\n",
       "  'dr': -25.0,\n",
       "  'u': 22.52512000000001},\n",
       " '1,1': {'ur': 38.00800000000001,\n",
       "  'r': 38.00800000000001,\n",
       "  'dr': -25.0,\n",
       "  'u': 29.40640000000001},\n",
       " '0,2': {'ur': -25.0,\n",
       "  'r': -25.0,\n",
       "  'dr': 48.760000000000005,\n",
       "  'u': 38.00800000000001},\n",
       " '1,2': {'ur': -25.0,\n",
       "  'r': 48.760000000000005,\n",
       "  'u': 38.00799905884625,\n",
       "  'dr': 48.760000000000005},\n",
       " '2,1': {'r': -25.0,\n",
       "  'dr': -25.0,\n",
       "  'ur': 38.00800000000001,\n",
       "  'u': 29.40345936000001},\n",
       " '1,3': {'ur': -25.0, 'r': 62.2, 'dr': -25.0, 'u': -25.0},\n",
       " '1,4': {'dr': 79.0, 'ur': -25.0, 'r': 79.0, 'u': -25.0},\n",
       " '2,5': {'ur': -25.0, 'u': 79.0, 'dr': 79.0, 'r': 100.0},\n",
       " '1,5': {'dr': 100.0, 'ur': -25.0, 'r': -25.0, 'u': -24.99999999975},\n",
       " '1,0': {'ur': 25.96585475360945,\n",
       "  'dr': 29.40640000000001,\n",
       "  'r': 29.37699360000001},\n",
       " '0,1': {'u': -4.999798870174921,\n",
       "  'ur': 32.10930000000001,\n",
       "  'r': 32.12538021000001,\n",
       "  'dr': 38.00800000000001},\n",
       " '2,3': {'ur': 62.2, 'dr': -0.9, 'r': -22.5},\n",
       " '3,4': {'r': -22.5},\n",
       " '3,6': {'r': 78.99999918109567,\n",
       "  'dr': 49.75787579806734,\n",
       "  'ur': 100.0,\n",
       "  'u': 90.0},\n",
       " '4,6': {'dr': -4.999966286709147,\n",
       "  'r': -4.999966286709147,\n",
       "  'u': 78.18112713836068}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0 -1.0 -1.0 -25.0 -25.0 -25.0 -25.0]\n",
      " [-1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -25.0]\n",
      " [-1.0 -1.0 -25.0 -1.0 -25.0 -1.0 100.0]\n",
      " [-1.0 -25.0 -25.0 -1.0 -1.0 -25.0 -1.0]\n",
      " [-1.0 -1.0 -1.0 -25.0 -25.0 -25.0 -1.0]]\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)}, threshold=sys.maxsize)\n",
    "\n",
    "# Global variables\n",
    "EPS = 0.1\n",
    "ALPHA = 0.9\n",
    "GAMMA = 0.8\n",
    "GOAL = (2,6)\n",
    "START = (2,0)\n",
    "WORLD = -1. * np.ones((5,7))\n",
    "OBSTACLES = [(0,3),(0,4),(0,5),(0,6),(1,6),(2,2),(2,4),(3,1),(3,2),(3,5),(4,3),(4,4),(4,5)]\n",
    "ACTIONS = {\n",
    "    'ur': [('ur', 0.8), ('u', 0.1), ('r', 0.1)],\n",
    "    'r': [('r', 0.8), ('ur', 0.1), ('dr', 0.1)], \n",
    "    'dr': [('dr', 0.8), ('r', 0.1), ('d', 0.1)]\n",
    "}\n",
    "\n",
    "# Populate world\n",
    "WORLD[GOAL] = 100\n",
    "\n",
    "for obstacle in OBSTACLES:\n",
    "    WORLD[obstacle] = -25\n",
    "\n",
    "print(WORLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move\n",
    "def move(state, action):\n",
    "    new_state = state.copy()\n",
    "    \n",
    "    # Right\n",
    "    if action == \"r\":\n",
    "        new_state[\"x\"] += 1\n",
    "    # Lower right\n",
    "    elif action == \"dr\":\n",
    "        new_state[\"x\"] += 1\n",
    "        new_state[\"y\"] += 1\n",
    "    # Down\n",
    "    elif action == \"d\":\n",
    "        new_state[\"y\"] += 1\n",
    "    # Lower left\n",
    "    elif action == \"dl\":\n",
    "        new_state[\"x\"] -= 1\n",
    "        new_state[\"y\"] += 1\n",
    "    # Left\n",
    "    elif action == \"l\":\n",
    "        new_state[\"x\"] -= 1\n",
    "    # Upper left\n",
    "    elif action == \"ul\":\n",
    "        new_state[\"x\"] -= 1\n",
    "        new_state[\"y\"] -= 1\n",
    "    # Up\n",
    "    elif action == \"u\":\n",
    "        new_state[\"y\"] -= 1\n",
    "    # Upper right\n",
    "    if action == \"ur\":\n",
    "        new_state[\"x\"] += 1\n",
    "        new_state[\"y\"] -= 1\n",
    "\n",
    "    return new_state\n",
    "\n",
    "# Check if state out of bound\n",
    "# and returns truncated state\n",
    "def truncate(state):\n",
    "    # Truncated state\n",
    "    new_state = state.copy()\n",
    "    \n",
    "    # Check if out of bounds\n",
    "    if new_state['y'] < 0:\n",
    "        new_state['y'] += 1\n",
    "        \n",
    "    if new_state['y'] >= WORLD.shape[0]:\n",
    "        new_state['y'] -= 1\n",
    "            \n",
    "    if new_state['x'] < 0:\n",
    "        new_state['x'] += 1\n",
    "        \n",
    "    if new_state['x'] >= WORLD.shape[1]:\n",
    "        new_state['x'] -= 1\n",
    "    \n",
    "    return new_state\n",
    "\n",
    "# Check if state terminal\n",
    "def terminal(state):\n",
    "    return True if (state['y'], state['x']) == GOAL or \\\n",
    "                   (state['y'], state['x']) in OBSTACLES \\\n",
    "                else False\n",
    "\n",
    "# Step function\n",
    "def step(state, action):\n",
    "    # Returns\n",
    "    done = None\n",
    "    reward = None\n",
    "    \n",
    "    # Move based on action\n",
    "    new_state = move(state, action)\n",
    "    \n",
    "    # Check that state is valid\n",
    "    new_state = truncate(new_state)\n",
    "    \n",
    "    # Check if state is terminal\n",
    "    done = terminal(new_state)\n",
    "    \n",
    "    # Get reward for new state\n",
    "    reward = WORLD[new_state['y'], new_state['x']]\n",
    "    \n",
    "    return (new_state, reward, done)\n",
    "\n",
    "def generate_action():\n",
    "    action = None\n",
    "    prob = np.random.uniform(0.0, 1.0)\n",
    "    \n",
    "    # Apply policy\n",
    "    if prob < 0.5:\n",
    "        action = 'ur' if prob > 0.25 else 'dr'\n",
    "    else:\n",
    "        action = 'r'\n",
    "        \n",
    "    # Apply non-det. action\n",
    "    prob = np.random.uniform(0.0, 1.0)\n",
    "    if prob < ACTIONS[action][0][1]:\n",
    "        action = ACTIONS[action][0][0]\n",
    "    else:\n",
    "        action = (ACTIONS[action][1][0] \n",
    "                  if prob > ACTIONS[action][1][1] \n",
    "                  else ACTIONS[action][2][0])\n",
    "    return action\n",
    "\n",
    "\n",
    "def TD_eval(loops=1000):\n",
    "    V = np.zeros_like(WORLD)\n",
    "\n",
    "    for count in range(loops): \n",
    "        s = {'y': START[0], 'x': START[1]}\n",
    "\n",
    "        # Episode loop\n",
    "        while True:\n",
    "            action = generate_action()\n",
    "            n_s, reward, done = step(s, action)\n",
    "                        \n",
    "            V[s['y'], s['x']] = (V[s['y'], s['x']] + ALPHA * \n",
    "                                (reward + GAMMA*V[n_s['y'], n_s['x']]-V[s['y'], s['x']]))\n",
    "            s = n_s\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-15.5, -20.4, -24.9, 0.0, 0.0, 0.0, 0.0],\n",
       "       [-13.7, -24.9, 25.7, -18.5, -16.5, -13.7, 0.0],\n",
       "       [-20.9, -24.9, 0.0, -18.4, 0.0, 88.7, 0.0],\n",
       "       [0.0, 0.0, 0.0, 0.0, -25.0, 0.0, 90.4],\n",
       "       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = TD_eval()\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_arrow(action):\n",
    "    if action == 'r':\n",
    "        return u'\\u2192'\n",
    "    elif action == 'l':\n",
    "        return u'\\u2190'\n",
    "    elif action == 'u':\n",
    "        return u'\\u2191'\n",
    "    elif action == 'd':\n",
    "        return u'\\u2193'\n",
    "    elif action == 'ul':\n",
    "        return u'\\u2196'\n",
    "    elif action == 'ur':\n",
    "        return u'\\u2197'\n",
    "    elif action == 'dr':\n",
    "        return u'\\u2198'\n",
    "    else:\n",
    "        return u'\\u2199'\n",
    "\n",
    "def eps_greedy(Q, s):\n",
    "    # Action from policy\n",
    "    action = None\n",
    "    \n",
    "    # Encode state\n",
    "    key_s = ','.join(str(x) for x in s.values())\n",
    "        \n",
    "    if key_s in Q.keys() and Q[key_s]:\n",
    "        if np.random.uniform(0.0, 1.0) > EPS: \n",
    "            action = max(Q[key_s].items(), key=operator.itemgetter(1))[0]\n",
    "        else:\n",
    "            action = generate_action()\n",
    "    else:\n",
    "        action = generate_action()\n",
    "    \n",
    "    # Create entry if not existent\n",
    "    if key_s not in Q.keys():\n",
    "        Q[key_s] = {}\n",
    "    if action not in Q[key_s].keys():\n",
    "        Q[key_s][action] = 0\n",
    "    \n",
    "    return action\n",
    "\n",
    "def max_action(Q, s, value=True):\n",
    "    # Action from policy\n",
    "    action = None\n",
    "    \n",
    "    # Encode state\n",
    "    key_s = ','.join(str(x) for x in s.values())\n",
    "        \n",
    "    if key_s in Q.keys() and Q[key_s]:\n",
    "        # Get max action\n",
    "        action = max(Q[key_s].items(), key=operator.itemgetter(1))[0]\n",
    "        \n",
    "        if value:\n",
    "            return Q[key_s][action]\n",
    "        else:\n",
    "            return action\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def show_policy(Q):\n",
    "    # Policy\n",
    "    P = WORLD.copy().astype(object)\n",
    "    \n",
    "    P[P == -1] = ' '\n",
    "    P[P == -25] = 'X'\n",
    "    P[P == 100] = '@'\n",
    "\n",
    "    \n",
    "    # Loop over all states\n",
    "    for key in Q.keys():\n",
    "        # Get state representations\n",
    "        state_list = key.split(\",\")\n",
    "        state = (int(state_list[0]), int(state_list[1]))\n",
    "        state_dict = {'y': int(state_list[0]), 'x': int(state_list[1])}\n",
    "        \n",
    "        # Get max action for state\n",
    "        action = max_action(Q, state_dict, value=False)\n",
    "        \n",
    "        # Insert policy action\n",
    "        P[state] = which_arrow(action)\n",
    "    \n",
    "    return P\n",
    "\n",
    "def q_2_v(Q):\n",
    "    V = WORLD.copy()\n",
    "    \n",
    "    for y in range(V.shape[0]):\n",
    "        for x in range(V.shape[1]):\n",
    "            s = {'y': y, 'x':x}\n",
    "            key_s = ','.join(str(x) for x in s.values())\n",
    "            if key_s not in Q.keys():  \n",
    "                continue\n",
    "            a_max = max(Q[key_s].items(), key=operator.itemgetter(1))[0]\n",
    "            actions = [(a_max, 1-EPS), (generate_action(), EPS)]\n",
    "\n",
    "            for a, p in actions: \n",
    "                n_s, r, done = step(s, a)\n",
    "                q_s_a = Q[key_s][a] if key_s in Q.keys() and a in Q[key_s].values() and Q[key_s] else 0\n",
    "                V[y,x] += p * (q_s_a + ALPHA * (r + GAMMA * max_action(Q, n_s) - q_s_a))\n",
    "        \n",
    "    return V\n",
    "            \n",
    "            \n",
    "            \n",
    "# Q-Learning\n",
    "def q_learning(loops=10000):\n",
    "    # Action value function\n",
    "    Q = {}\n",
    "    \n",
    "    for count in range(loops):\n",
    "        # Initial state\n",
    "        s = {'y': START[0], 'x': START[1]}\n",
    "\n",
    "        # Episode loop\n",
    "        while True:\n",
    "            action = None\n",
    "            \n",
    "            # EPS greedy\n",
    "            action = eps_greedy(Q, s)\n",
    "\n",
    "            # Get next state, reward and if terminal    \n",
    "            n_s, r, done = step(s, action)\n",
    "            \n",
    "            # Encode state\n",
    "            key_s = ','.join(str(x) for x in s.values())\n",
    "            \n",
    "            # Update Q\n",
    "            Q[key_s][action] = (Q[key_s][action] +\n",
    "                              ALPHA * (r + GAMMA * max_action(Q, n_s) - Q[key_s][action]))\n",
    "            \n",
    "            # Update state\n",
    "            s = n_s\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "    return Q, q_2_v(Q), show_policy(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, V, policy = q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original World: \n",
      " [[-1.0 -1.0 -1.0 -25.0 -25.0 -25.0 -25.0]\n",
      " [-1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -25.0]\n",
      " [-1.0 -1.0 -25.0 -1.0 -25.0 -1.0 100.0]\n",
      " [-1.0 -25.0 -25.0 -1.0 -1.0 -25.0 -1.0]\n",
      " [-1.0 -1.0 -1.0 -25.0 -25.0 -25.0 -1.0]]\n",
      "\n",
      "Q-Learning V(s): \n",
      " [[-1.0 33.2 36.2 -25.0 -25.0 -25.0 -25.0]\n",
      " [25.5 27.5 41.9 47.1 60.7 77.8 -25.0]\n",
      " [25.5 33.2 -25.0 55.0 -25.0 89.0 100.0]\n",
      " [-1.0 -25.0 -25.0 -1.0 -23.5 -25.0 87.1]\n",
      " [-1.0 -1.0 -1.0 -25.0 -25.0 -25.0 70.1]]\n",
      "\n",
      "Q-Learning policy: \n",
      " [[' ' '↘' '↘' 'X' 'X' 'X' 'X']\n",
      " ['→' '↗' '→' '→' '↘' '↘' 'X']\n",
      " ['→' '↗' 'X' '↗' 'X' '→' '@']\n",
      " [' ' 'X' 'X' ' ' '↑' 'X' '↗']\n",
      " [' ' ' ' ' ' 'X' 'X' 'X' '↗']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original World: \\n\", WORLD, end='\\n\\n')\n",
    "print(\"Q-Learning V(s): \\n\", V, end='\\n\\n')\n",
    "print(\"Q-Learning policy: \\n\", policy, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Check the image 'policy.png' to visualize the policy correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0 -10.0\n",
      "  -10.0 -10.0]\n",
      " [-10.0 -10.0 -10.0 -10.0 -10.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "  -1.0]\n",
      " [-10.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0]\n",
      " [-10.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0]\n",
      " [-10.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0]\n",
      " [-10.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0]\n",
      " [-10.0 -10.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -10.0 -10.0\n",
      "  -10.0]\n",
      " [-10.0 -10.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -10.0 -10.0 -10.0 -10.0\n",
      "  -10.0]\n",
      " [-10.0 -10.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -10.0 -10.0 -10.0 -10.0 -10.0\n",
      "  -10.0]\n",
      " [-10.0 -10.0 -10.0 -1.0 -1.0 -1.0 -1.0 -1.0 -10.0 -10.0 -10.0 -10.0\n",
      "  -10.0 -10.0]\n",
      " [-10.0 -10.0 -10.0 -1.0 -1.0 -1.0 -1.0 -1.0 -10.0 -10.0 -10.0 -10.0\n",
      "  -10.0 -10.0]\n",
      " [-10.0 -10.0 -10.0 -10.0 -1.0 -1.0 -1.0 -1.0 -10.0 -10.0 -10.0 -10.0\n",
      "  -10.0 -10.0]\n",
      " [-10.0 -10.0 -10.0 -10.0 -1.0 -1.0 -1.0 -1.0 -10.0 -10.0 -10.0 -10.0\n",
      "  -10.0 -10.0]\n",
      " [-10.0 -10.0 -10.0 -10.0 -1.0 -1.0 -1.0 -10.0 -10.0 -10.0 -10.0 -10.0\n",
      "  -10.0 -10.0]\n",
      " [-10.0 -10.0 -10.0 -10.0 -1.0 -1.0 -1.0 -10.0 -10.0 -10.0 -10.0 -10.0\n",
      "  -10.0 -10.0]\n",
      " [-10.0 -10.0 -10.0 -1.0 -1.0 -1.0 -1.0 -10.0 -10.0 -10.0 -10.0 -10.0\n",
      "  -10.0 -10.0]\n",
      " [-10.0 -10.0 -10.0 -1.0 -1.0 -1.0 -1.0 -10.0 -10.0 -10.0 -10.0 -10.0\n",
      "  -10.0 -10.0]\n",
      " [-10.0 -10.0 -10.0 -1.0 -1.0 -1.0 -1.0 -10.0 -10.0 -10.0 -10.0 -10.0\n",
      "  -10.0 -10.0]]\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)}, threshold=sys.maxsize)\n",
    "# Global variables\n",
    "WORLD = -1. * np.ones((18,14))\n",
    "STARTS = [(17,3), (17,4), (17,5), (17,6)]\n",
    "GOALS = [(1,13), (2,13), (3,13), (4,13), (5,13)]\n",
    "BEG_ = [(6,11),(7,9),(8,8),(9,8),(10,8),(11,8),(12,8),(13,7),(14,7),(15,7),(16,7),(17,7)]\n",
    "END_ = [(0,13),(1,4),(2,0),(3,0),(4,0),(5,0),(6,1),(7,1),(8,1),(9,2),(10,2),(11,3),(12,3),(13,3),(14,3),(15,2),(16,2),(17,2)]\n",
    "\n",
    "# Populate world\n",
    "for coup in END_:\n",
    "    WORLD[coup[0], 0:coup[1]+1] = -10\n",
    "    \n",
    "for coup in BEG_:\n",
    "    WORLD[coup[0], coup[1]:14] = -10\n",
    "\n",
    "for cell in GOALS:\n",
    "    WORLD[cell] = -1\n",
    "\n",
    "print(WORLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "MIN = 1\n",
    "MAX = 5\n",
    "EPS = 0.4\n",
    "\n",
    "def generate_actions():\n",
    "    actions = []\n",
    "    for i in range(MAX+1):\n",
    "        for j in range(MAX+1):\n",
    "            if MIN <= (i + j) <= MAX:\n",
    "                actions.append((i,j))\n",
    "    return actions\n",
    "\n",
    "def terminal(state, allow=False):\n",
    "    # Out of bounds\n",
    "    if ((state['y'] < 0 or state['y'] >= WORLD.shape[0]) or \n",
    "        (state['x'] < 0 or state['x'] >= WORLD.shape[1])):\n",
    "        return True\n",
    "    \n",
    "    # Goal state\n",
    "    elif ((state['y'], state['x']) in GOALS) and not allow:\n",
    "        return True\n",
    "    \n",
    "    # Obstacle hit\n",
    "    elif WORLD[state['y'], state['x']] == -10:\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def move(state, action):\n",
    "    new_state = state.copy()\n",
    "    \n",
    "    # One of the velocity is 0\n",
    "    if action[0] == 0 or action[1] == 0:        \n",
    "        # Index of non-zero velocity\n",
    "        idx = 0 if action[1] == 0 else 1\n",
    "        \n",
    "        # Dictionary entry based on idx\n",
    "        k = \"x\" if action[1] == 0 else \"y\"\n",
    "        \n",
    "        # Simulate trajectory\n",
    "        for x in range(action[idx]):\n",
    "            if k == \"x\":\n",
    "                new_state[k] += 1\n",
    "            else:\n",
    "                new_state[k] -= 1\n",
    "            \n",
    "            # Check if terminal\n",
    "            if terminal(new_state):\n",
    "                return new_state, True\n",
    "            \n",
    "    # Diagonal movement\n",
    "    else:\n",
    "        if action[0] >= action[1]:\n",
    "            # Simulate trajectory\n",
    "            for x in range(action[0]):\n",
    "                new_state['y'] -= 1\n",
    "                new_state['x'] += 1\n",
    "\n",
    "                # Check if terminal\n",
    "                if terminal(new_state):\n",
    "                    return new_state, True\n",
    "        else:\n",
    "            # Simulate trajectory\n",
    "            for x in range(action[0]):\n",
    "                new_state['y'] -= 1\n",
    "                new_state['x'] += 1\n",
    "\n",
    "                # Check if terminal\n",
    "                if terminal(new_state):\n",
    "                    return new_state, True\n",
    "            \n",
    "            for x in range(action[1] - action[0]):\n",
    "                new_state['y'] -= 1\n",
    "                \n",
    "                # Check if terminal\n",
    "                if terminal(new_state):\n",
    "                    return new_state, True\n",
    "                \n",
    "    return new_state, False\n",
    "\n",
    "def encode(state, action, R, Q, Pi):\n",
    "    # Enconding key\n",
    "    key_s = ','.join(str(x) for x in state.values())\n",
    "    key_a = ','.join(str(x) for x in action)\n",
    "        \n",
    "    # Check if key not already in dict\n",
    "    if key_s not in R.keys():\n",
    "        R[key_s] = {}\n",
    "    \n",
    "    if key_s not in Q.keys():\n",
    "        Q[key_s] = {}\n",
    "        \n",
    "    if key_s not in Pi.keys():\n",
    "        Pi[key_s] = {}\n",
    "    \n",
    "    if key_a not in R[key_s].keys():\n",
    "        R[key_s][key_a] = []\n",
    "        \n",
    "    return key_s, key_a\n",
    "\n",
    "def action_selection(action, actions):\n",
    "    \n",
    "    new_action = ()\n",
    "    while True:\n",
    "        # Random velocity increments\n",
    "        x_increment = np.random.randint(-1,2)\n",
    "        y_increment = np.random.randint(-1,2)\n",
    "\n",
    "        # Compute new velocity\n",
    "        new_action = (action[0] + x_increment, \n",
    "                      action[1] + y_increment)\n",
    "        \n",
    "        if new_action in actions:\n",
    "            break\n",
    "    \n",
    "    return new_action\n",
    "\n",
    "def update_episode(episode, state, action, R, Q, Pi):\n",
    "    # Encode state and action\n",
    "    key_s, key_a = encode(state, action, R, Q, Pi)\n",
    "    \n",
    "    # Add new episode\n",
    "    episode.append((key_s, key_a))\n",
    "\n",
    "def admissible_actions(actions, key_s):\n",
    "    # Get state from key\n",
    "    s_list = key_s.split(',')\n",
    "    state = {\"y\": int(s_list[0]), \"x\": int(s_list[1])}\n",
    "    \n",
    "    # Admissible actions\n",
    "    admissible = []\n",
    "        \n",
    "    for action in actions:        \n",
    "        # Get new state with action\n",
    "        next_state, _ = move(state, action)\n",
    "        \n",
    "        # Check if state is admissible\n",
    "        if not terminal(next_state, allow=True):\n",
    "            admissible.append(action)\n",
    "    \n",
    "    return admissible\n",
    "\n",
    "def world_admissible_actions(WORLD):\n",
    "    # Admissible actions\n",
    "    A = {}\n",
    "    \n",
    "    # Possible actions\n",
    "    actions = generate_actions()\n",
    "    \n",
    "    for y in range(WORLD.shape[0]):\n",
    "        for x in range(WORLD.shape[1]):\n",
    "            if (y,x) not in GOALS and WORLD[y,x] != -10:\n",
    "                # Creat key for state\n",
    "                key_s = str(y) + \",\" + str(x)\n",
    "                A[key_s] = admissible_actions(actions, key_s)\n",
    "    \n",
    "    return A\n",
    "\n",
    "def monte_carlo(loops=1000, uniform=False):\n",
    "    # Value functions\n",
    "    R = {}\n",
    "    Q = {}\n",
    "    Pi = {}\n",
    "    V = np.zeros_like(WORLD)\n",
    "    count = 0\n",
    "    # Possible actions\n",
    "    actions = generate_actions()\n",
    "    \n",
    "    # World admissible actions\n",
    "    A = world_admissible_actions(WORLD)\n",
    "    \n",
    "    for count in range(loops):        \n",
    "        # Starting state\n",
    "        state = {'y': 17, 'x': np.random.randint(3,7)}\n",
    "        \n",
    "        # Episode states\n",
    "        episode = []\n",
    "        \n",
    "        # Initial action\n",
    "        action = (0,1)\n",
    "        \n",
    "        # Update first episode\n",
    "        update_episode(episode, state, action, R, Q, Pi)\n",
    "        \n",
    "        # Apply move\n",
    "        state, _ = move(state, action)\n",
    "                \n",
    "        ## (a)\n",
    "        while True:\n",
    "            # Action choice\n",
    "            if uniform:\n",
    "                action = action_selection(action, actions)\n",
    "            else: \n",
    "                key_s = ','.join(str(x) for x in state.values())\n",
    "                \n",
    "                if key_s in Pi.keys() and Pi[key_s]:\n",
    "                    if np.random.uniform(0.0, 1.0) >= (EPS / len(Pi[key_s])): \n",
    "                        a_star = max(Pi[key_s].items(), key=operator.itemgetter(1))[0]\n",
    "                        a_list = a_star.split(',')\n",
    "                        action = (int(a_list[0]), int(a_list[1]))\n",
    "                    else:\n",
    "                        action = action_selection(action, actions)\n",
    "                else:\n",
    "                    action = action_selection(action, actions)\n",
    "            \n",
    "            # Update episode with action\n",
    "            update_episode(episode, state, action, R, Q, Pi)\n",
    "            \n",
    "            # Apply move\n",
    "            state, is_terminal = move(state, action)\n",
    "            \n",
    "            # End episode if terminal state\n",
    "            if is_terminal:\n",
    "                break\n",
    "        \n",
    "        # (b)\n",
    "        for key_s, key_a in episode:\n",
    "            # Get action\n",
    "            a_list = key_a.split(',')\n",
    "            action = (int(a_list[0]), int(a_list[1]))\n",
    "            \n",
    "            # Get resulting state with given action\n",
    "            s_list = key_s.split(',')\n",
    "            state = {\"y\": int(s_list[0]), \"x\": int(s_list[1])}\n",
    "            next_state, _ = move(state, action)\n",
    "                \n",
    "            # Update return dict\n",
    "            R[key_s][key_a].append(WORLD[next_state[\"y\"], next_state[\"x\"]])\n",
    "            Q[key_s][key_a] = np.mean(R[key_s][key_a])\n",
    "            \n",
    "        # (c)\n",
    "        if not uniform:\n",
    "            for key_s, _ in episode:\n",
    "                a_star = max(Q[key_s].items(), key=operator.itemgetter(1))[0]\n",
    "                a_len = len(Q[key_s])\n",
    "                for key_a in Q[key_s].keys():\n",
    "                    Pi[key_s][key_a] = (1 - EPS + EPS / a_len) if a_star == key_a else EPS / a_len\n",
    "    \n",
    "    # Compute value state table\n",
    "    if uniform:\n",
    "        for key in Q.keys():\n",
    "            # Get state\n",
    "            s_list = key.split(',')\n",
    "            state = (int(s_list[0]), int(s_list[1]))\n",
    "                        \n",
    "            # Values for the state\n",
    "            values = Q[key].values()\n",
    "                        \n",
    "            # Update table\n",
    "            V[int(s_list[0]), int(s_list[1])] = sum(values) * (1/len(values))\n",
    "        \n",
    "    return (Pi, Q, R) if not uniform else (V, Q, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, Q, R = monte_carlo(loops=2000, uniform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Value table for 1000 episodes with u.a.r action selection: \n",
      "\n",
      "\n",
      "[[0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 0.0 -10.0 -10.0 -9.2 -8.6 -10.0 -9.1 -8.9 -7.8 0.0]\n",
      " [0.0 0.0 0.0 0.0 -10.0 -10.0 -9.0 -8.5 -7.0 -8.4 -6.7 -6.4 -1.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 -5.5 -7.0 -5.1 -5.5 -4.5 -4.6 -5.5 -2.3 -1.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 -7.8 -4.0 -3.2 -2.9 -2.8 -1.0 -1.0 -1.0 -1.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 -3.2 -1.8 -1.0 -1.6 -1.0 -1.0 -1.0 -1.0 -1.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 -2.5 -1.0 -1.0 -1.0 -1.6 -2.0 -7.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 -1.0 -1.0 -1.0 -1.6 -2.8 0.0 0.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 -1.0 -1.0 -1.7 -2.2 0.0 0.0 0.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 -1.0 -1.0 -1.7 -8.2 0.0 0.0 0.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 -1.0 -1.0 -4.2 -8.2 0.0 0.0 0.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 -1.0 -1.0 -3.8 -8.2 0.0 0.0 0.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 -1.0 -1.0 -5.5 -8.5 0.0 0.0 0.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 -1.0 -1.8 -6.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 0.0 -1.0 -1.8 -7.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 -4.6 -1.0 -4.4 -8.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 -2.8 -1.0 -3.6 -8.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]\n",
      " [0.0 0.0 0.0 -1.0 -1.0 -1.0 -1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"State-Value table for 1000 episodes with u.a.r action selection: \\n\\n\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi, Q, R = monte_carlo(loops=1000, uniform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory 0 :\n",
      "\n",
      " [('17,3', '0,1'), ('16,3', '1,1'), ('15,4', '0,1'), ('14,4', '0,2'), ('12,4', '1,2'), ('10,5', '2,3'), ('7,7', '3,2'), ('4,10', '2,3'), ('1,12', '3,2'), ('0,13', '3,2')] \n",
      "\n",
      "Trajectory 1 :\n",
      "\n",
      " [('17,4', '0,1'), ('16,4', '0,2'), ('14,4', '0,2'), ('12,4', '1,2'), ('10,5', '2,3'), ('7,7', '3,2'), ('4,10', '2,3'), ('1,12', '3,2'), ('0,13', '3,2')] \n",
      "\n",
      "Trajectory 2 :\n",
      "\n",
      " [('17,5', '0,1'), ('16,5', '1,0'), ('16,6', '0,1'), ('15,6', '0,1'), ('14,6', '0,1'), ('13,6', '0,3'), ('10,6', '1,2'), ('8,7', '0,1'), ('7,7', '3,2'), ('4,10', '2,3'), ('1,12', '3,2'), ('0,13', '3,2')] \n",
      "\n",
      "Trajectory 3 :\n",
      "\n",
      " [('17,6', '0,1'), ('16,6', '0,1'), ('15,6', '0,1'), ('14,6', '0,1'), ('13,6', '0,3'), ('10,6', '1,2'), ('8,7', '0,1'), ('7,7', '3,2'), ('4,10', '2,3'), ('1,12', '3,2'), ('0,13', '3,2')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "trajectories = []\n",
    "\n",
    "for state_ in STARTS:\n",
    "    # Episode\n",
    "    episode = []\n",
    "    \n",
    "    # States\n",
    "    state = {'y': state_[0], 'x': state_[1]}\n",
    "    count =0\n",
    "    while True:\n",
    "        # Transform state to key\n",
    "        key_s = ','.join(str(x) for x in [state['y'],state['x']])\n",
    "        \n",
    "        # Fetch best action\n",
    "        try:\n",
    "            a_star = max(Pi[key_s].items(), key=operator.itemgetter(1))[0]\n",
    "        except:\n",
    "            print(key_s)\n",
    "            print(Pi[key_s])\n",
    "        a_list = a_star.split(',')\n",
    "        action = (int(a_list[0]), int(a_list[1]))\n",
    "        # Apply move to action\n",
    "        new_state, is_terminal = move(state, action)\n",
    "        \n",
    "        # Enconding keys\n",
    "        key_s = ','.join(str(x) for x in state.values())\n",
    "        key_a = ','.join(str(x) for x in action)\n",
    "\n",
    "        episode.append((key_s, key_a))\n",
    "        # Check new state terminal\n",
    "        if is_terminal:\n",
    "            # Enconding keys goal\n",
    "            key_s = ','.join(str(x) for x in new_state.values())\n",
    "            key_a = ','.join(str(x) for x in action)\n",
    "\n",
    "            episode.append((key_s, key_a))\n",
    "            break\n",
    "        else:\n",
    "            state = new_state\n",
    "\n",
    "            \n",
    "    trajectories.append(episode)\n",
    "    \n",
    "for count, trajectory in enumerate(trajectories):\n",
    "    print(\"Trajectory \" + str(count) + \" :\\n\\n\", trajectory, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

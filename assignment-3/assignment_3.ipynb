{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "solution_task1 = np.load('data/stateValuesTask1.npy')\n",
    "grid_world = np.load('data/gridworld.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOAL_S = {'x':8,'y':3}\n",
    "\n",
    "def is_outside(state):\n",
    "    return ((state['y'] < 0 or state['y'] >= grid_world.shape[0]) \n",
    "              or (state['x'] < 0 or state['x'] >= grid_world.shape[1]))\n",
    "\n",
    "def terminal_state(state, goal_inc=True):\n",
    "    # Entered a goal state\n",
    "    if goal_inc and state == GOAL_S:\n",
    "        return True\n",
    "    # Exeeded boundary limits\n",
    "    elif is_outside(state):\n",
    "        return True\n",
    "    # Entered a cell X\n",
    "    elif grid_world[state['y'], state['x']] == -20:\n",
    "        return True\n",
    "\n",
    "    # Not a terminal state\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_reward(state):\n",
    "    if is_outside(state):\n",
    "        return -30;\n",
    "    else:\n",
    "        return grid_world[state['y'], state['x']]\n",
    "\n",
    "def make_move(state, action, correction=True):\n",
    "    state_c = state.copy()\n",
    "    \n",
    "    if action == 'l':\n",
    "        state_c['x']  -= 1\n",
    "    elif action == 'r':\n",
    "        state_c['x']  += 1\n",
    "    elif action == 'u':\n",
    "        state_c['y']  -= 1\n",
    "    elif action == 'd':\n",
    "        state_c['y']  += 1\n",
    "        \n",
    "    if not terminal_state(state_c, goal_inc=False) or not correction:\n",
    "        state['x'] = state_c['x']\n",
    "        state['y'] = state_c['y']\n",
    "        \n",
    "    return get_reward(state_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.007\n",
    "actions = [('r', 0.625, u'\\u2192'), ('l', 0.125, u'\\u2190'), ('u', 0.125, u'\\u2191'), ('d', 0.125, u'\\u2193')]\n",
    "\n",
    "def compute_V(disc=0.9):\n",
    "    # Value function matrix\n",
    "    V = np.zeros_like(grid_world)\n",
    "    # Delta value\n",
    "    delta = np.inf\n",
    "    \n",
    "    while delta > THRESHOLD:\n",
    "        # Reset delta\n",
    "        delta  = 0\n",
    "\n",
    "        # Copy of V\n",
    "        V_copy = V.copy()\n",
    "        \n",
    "        # States loop\n",
    "        for y in range(grid_world.shape[0]):\n",
    "            for x in range(grid_world.shape[1]):\n",
    "                # Current state\n",
    "                state = {'x':x,'y':y}\n",
    "                \n",
    "                if terminal_state(state):\n",
    "                    continue\n",
    "                                \n",
    "                # Temporary value for the state\n",
    "                V_prev = V[y,x]\n",
    "                \n",
    "                # Compute new value\n",
    "                V[y,x] = 0\n",
    "                for action, probability, _ in actions:\n",
    "                    state_a = state.copy()\n",
    "                    reward = make_move(state_a, action)\n",
    "                    V[y,x] += probability * (reward + disc * V_copy[state_a['y'], state_a['x']])\n",
    "                \n",
    "                # Compute new delta\n",
    "                delta = max(delta, np.abs(V_prev - V[y,x]))\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulative difference from the solution results: 0.34249411007374775\n",
      "\n",
      "[[ -55.72233031  -60.28711964  -76.2187774   -85.94227143  -95.17265171\n",
      "  -102.06584518 -109.1497957  -128.03694587 -154.35820998]\n",
      " [ -70.23910801  -71.03512149  -82.42332396  -97.04567539 -113.44958832\n",
      "     0.         -138.84246494    0.         -140.12747124]\n",
      " [-114.35530129    0.            0.            0.            0.\n",
      "     0.         -156.42672419    0.          -71.57989163]\n",
      " [-113.49035193    0.          -67.70838784  -82.25522086 -104.2432133\n",
      "  -131.62855066    0.           67.97547745    0.        ]\n",
      " [ -66.87419882  -61.7022072   -77.43326267    0.            0.\n",
      "     0.          -51.56820765  -41.72566933  -63.11667048]\n",
      " [ -64.48159917  -63.86372895  -69.52992994  -77.89104824  -83.19067425\n",
      "   -88.81991069  -95.51570805    0.         -145.95801304]\n",
      " [ -81.40779974    0.            0.            0.            0.\n",
      "     0.            0.         -146.67829987 -167.44391259]\n",
      " [ -24.06403261  -19.82047396  -28.49113241  -42.30294539  -57.38418682\n",
      "   -75.05396632 -100.94960102 -122.14296902    0.        ]\n",
      " [ -29.37020784  -33.21728211  -49.02465928  -73.9060024     0.\n",
      "   -85.55182017 -105.09043508 -125.18325993    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "result_task1 = compute_V()\n",
    "diff = sum(abs(solution_task1 - result_task1).flatten())\n",
    "\n",
    "print(\"Accumulative difference from the solution results: {}\\n\\n{}\".format(diff, result_task1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = result_task1.copy()\n",
    "policy = V.copy().astype(object)\n",
    "\n",
    "# set goal cell value to inf\n",
    "V[GOAL_S['y'], GOAL_S['x']] = np.inf\n",
    "\n",
    "# Loop per every states\n",
    "for y in range(grid_world.shape[0]):\n",
    "    for x in range(grid_world.shape[1]):\n",
    "        state = {'x':x,'y':y}\n",
    "        state_policy = ''\n",
    "               \n",
    "        # Check if Goal state\n",
    "        if state == GOAL_S:\n",
    "            policy[state['y'],state['x']] = '@'\n",
    "            continue\n",
    "        # Check if terminal state\n",
    "        if terminal_state(state, goal_inc=True):\n",
    "            policy[state['y'],state['x']] = 'X'\n",
    "            continue\n",
    "            \n",
    "        # Get the most rewarding action\n",
    "        best_r_action = -np.inf\n",
    "        for action, _, arrow in actions:\n",
    "            state_a = state.copy()\n",
    "            reward = make_move(state_a, action, correction=False)\n",
    "            \n",
    "            if terminal_state(state_a, goal_inc=False):\n",
    "                continue \n",
    "            \n",
    "            a_move = V[state_a['y'], state_a['x']]\n",
    "            best_r_action = a_move if a_move > best_r_action else best_r_action\n",
    "           \n",
    "        # Assign arrow(s) (it can be that multiple\n",
    "        # states are sharing the same reward)\n",
    "        for action, _, arrow in actions:\n",
    "            state_a = state.copy()\n",
    "            reward = make_move(state_a, action)\n",
    "            \n",
    "            if terminal_state(state_a, goal_inc=False):\n",
    "                continue \n",
    "            \n",
    "            if V[state_a['y'], state_a['x']] == best_r_action:\n",
    "                state_policy += arrow\n",
    "        \n",
    "        # Insert the arrow in the policy\n",
    "        policy[state['y'],state['x']] = state_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['→', '←', '←', '←', '←', '←', '←', '←', '←'],\n",
       "       ['↑', '↑', '←', '←', '↑', 'X', '↑', 'X', '↓'],\n",
       "       ['↑', 'X', 'X', 'X', 'X', 'X', '↑', 'X', '↓'],\n",
       "       ['↓', 'X', '↓', '←', '←', '←', 'X', '→', '@'],\n",
       "       ['→', '↓', '←', 'X', 'X', 'X', '→', '↑', '↑'],\n",
       "       ['→', '↑', '←', '←', '←', '←', '↑', 'X', '↑'],\n",
       "       ['↓', 'X', 'X', 'X', 'X', 'X', 'X', '↓', '↑'],\n",
       "       ['→', '←', '←', '←', '←', '←', '←', '←', 'X'],\n",
       "       ['↑', '↑', '↑', '↑', 'X', '↑', '←', '←', 'X']], dtype=object)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

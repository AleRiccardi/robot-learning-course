{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Print options\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# Sanity check data\n",
    "grid_world = np.load('data/gridworld.npy')\n",
    "solution_task31 = np.load('data/stateValuesTask1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "DISCOUNT = 0.9\n",
    "THRESHOLD = 0.007\n",
    "GOAL_S = {'x':8,'y':3}\n",
    "actions = [('r', 0.625, u'\\u2192'), ('l', 0.125, u'\\u2190'), ('u', 0.125, u'\\u2191'), ('d', 0.125, u'\\u2193')]\n",
    "actions_diag = [('r', 0.5625, u'\\u2192'), ('l', 0.0625, u'\\u2190'), ('u', 0.0625, u'\\u2191'), ('d', 0.0625, u'\\u2193'), \n",
    "                ('lu', 0.0625, u'\\u2196'), ('ru', 0.0625, u'\\u2197'), ('rd', 0.0625, u'\\u2198'), ('ld', 0.0625, u'\\u2199')]\n",
    "\n",
    "# Common routines\n",
    "def is_outside(state):\n",
    "    return ((state['y'] < 0 or state['y'] >= grid_world.shape[0]) \n",
    "              or (state['x'] < 0 or state['x'] >= grid_world.shape[1]))\n",
    "\n",
    "def terminal_state(state, goal_inc=True):\n",
    "    # Entered a goal state\n",
    "    if goal_inc and state == GOAL_S:\n",
    "        return True\n",
    "    # Exeeded boundary limits\n",
    "    elif is_outside(state):\n",
    "        return True\n",
    "    # Entered a cell X\n",
    "    elif grid_world[state['y'], state['x']] == -20:\n",
    "        return True\n",
    "\n",
    "    # Not a terminal state\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_reward(state):\n",
    "    if is_outside(state):\n",
    "        return -30;\n",
    "    else:\n",
    "        return grid_world[state['y'], state['x']]\n",
    "\n",
    "def move(state, action, correction=True):\n",
    "    state_c = state.copy()\n",
    "    \n",
    "    if action == 'l':\n",
    "        state_c['x']  -= 1\n",
    "    elif action == 'r':\n",
    "        state_c['x']  += 1\n",
    "    elif action == 'u':\n",
    "        state_c['y']  -= 1\n",
    "    elif action == 'd':\n",
    "        state_c['y']  += 1\n",
    "        \n",
    "    if not terminal_state(state_c, goal_inc=False) or not correction:\n",
    "        state['x'] = state_c['x']\n",
    "        state['y'] = state_c['y']\n",
    "        \n",
    "    return get_reward(state_c)\n",
    "\n",
    "def extended_move(state, action, correction=True):\n",
    "    state_c = state.copy()\n",
    "    \n",
    "    if action == 'l':\n",
    "        state_c['x'] -= 1\n",
    "    elif action == 'r':\n",
    "        state_c['x'] += 1\n",
    "    elif action == 'u':\n",
    "        state_c['y'] -= 1\n",
    "    elif action == 'd':\n",
    "        state_c['y'] += 1\n",
    "    elif action == 'lu':\n",
    "        state_c['x'] -= 1\n",
    "        state_c['y'] -= 1\n",
    "    elif action == 'ru':\n",
    "        state_c['x'] += 1\n",
    "        state_c['y'] -= 1\n",
    "    elif action == 'ld':\n",
    "        state_c['x'] -= 1\n",
    "        state_c['y'] += 1\n",
    "    elif action == 'rd':\n",
    "        state_c['x'] += 1\n",
    "        state_c['y'] += 1\n",
    "        \n",
    "    if not terminal_state(state_c, goal_inc=False) or not correction:\n",
    "        state['x'] = state_c['x']\n",
    "        state['y'] = state_c['y']\n",
    "        \n",
    "    return get_reward(state_c)\n",
    "\n",
    "def get_actions(action):\n",
    "    if action == 'l':\n",
    "        return ['l', 'lu', 'ld']\n",
    "    if action == 'r':\n",
    "        return ['r', 'ru', 'rd']\n",
    "    if action == 'u':\n",
    "        return ['u', 'ru', 'lu']\n",
    "    if action == 'd':\n",
    "        return ['d', 'ld', 'rd']\n",
    "    if action == 'lu':\n",
    "        return ['lu', 'l', 'u']\n",
    "    if action == 'ld':\n",
    "        return ['ld', 'l', 'd']\n",
    "    if action == 'ru':\n",
    "        return ['ru', 'r', 'u']\n",
    "    if action == 'rd':\n",
    "        return ['rd', 'r', 'd']\n",
    "\n",
    "def non_deterministic_move(state, action, V):\n",
    "    #print(\"Original state: \", state)\n",
    "    \n",
    "    # Transitions probabilities\n",
    "    transitions = [0.8, 0.1, 0.1]\n",
    "    \n",
    "    # Rewards\n",
    "    rewards = []\n",
    "    \n",
    "    # States values\n",
    "    state_values = []\n",
    "    \n",
    "    # Get possible actions\n",
    "    actions = get_actions(action)\n",
    "    \n",
    "    # Populate actions holders\n",
    "    for l_action in actions:\n",
    "        # Copy of state\n",
    "        state_a = state.copy()\n",
    "        \n",
    "        # Populate lists\n",
    "        reward = extended_move(state_a, l_action)\n",
    "        value = V[state_a['y'], state_a['x']]\n",
    "        rewards.append(reward)\n",
    "        state_values.append(value)\n",
    "        #print(\"Action \", l_action, \" State obtained \", state_a, \" Reward \", reward, \" Value \", value)\n",
    "    \n",
    "    return (transitions, rewards, state_values)\n",
    "    \n",
    "def compute_state_value_function(actions, diag_move=False, deterministic=True, iter_lim=np.inf):\n",
    "    # Value function matrix\n",
    "    V = np.zeros_like(grid_world)\n",
    "    \n",
    "    # Delta value\n",
    "    delta = np.inf\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    while delta > THRESHOLD and count < iter_lim:\n",
    "        # Reset delta\n",
    "        delta  = 0\n",
    "\n",
    "        # Copy of V\n",
    "        V_copy = V.copy()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        # States loop\n",
    "        for y in range(grid_world.shape[0]):\n",
    "            for x in range(grid_world.shape[1]):\n",
    "                # Current state\n",
    "                state = {'x':x,'y':y}\n",
    "                \n",
    "                if terminal_state(state):\n",
    "                    continue\n",
    "        \n",
    "                # Temporary value for the state\n",
    "                V_prev = V[y,x]\n",
    "                \n",
    "                # Reset value\n",
    "                V[y,x] = 0\n",
    "                \n",
    "                # Compute new value (diag move allowed)\n",
    "                for action, probability, _ in actions:\n",
    "                    state_a = state.copy()\n",
    "                    \n",
    "                    if (not deterministic):\n",
    "                        trans, rewards, state_values = non_deterministic_move(state_a, action, V_copy)\n",
    "                        for z in range(3):\n",
    "                            V[y,x] += probability * (trans[z] * (rewards[z] + DISCOUNT * state_values[z]))\n",
    "                    else:\n",
    "                        reward = extended_move(state_a, action) if diag_move else move(state_a, action)\n",
    "                        V[y,x] += probability * (reward + DISCOUNT * V_copy[state_a['y'], state_a['x']])\n",
    "                \n",
    "                # Compute new delta\n",
    "                delta = max(delta, np.abs(V_prev - V[y,x]))\n",
    "\n",
    "    return V\n",
    "\n",
    "def policy(state_value_table, actions, diag_move=False):\n",
    "    # State-Value and Action-Value\n",
    "    # tables to be used and computed\n",
    "    V = state_value_table.copy()\n",
    "    policy = V.copy().astype(object)\n",
    "\n",
    "    # Set goal cell value to inf\n",
    "    V[GOAL_S['y'], GOAL_S['x']] = np.inf\n",
    "\n",
    "    # Loop over every state\n",
    "    for y in range(grid_world.shape[0]):\n",
    "        for x in range(grid_world.shape[1]):\n",
    "            # Current state\n",
    "            state = {'x':x,'y':y}\n",
    "            \n",
    "            # State policy\n",
    "            state_policy = ''\n",
    "\n",
    "            # Check if state is Goal\n",
    "            if state == GOAL_S:\n",
    "                policy[state['y'],state['x']] = '@'\n",
    "                continue\n",
    "                \n",
    "            # Check if terminal state\n",
    "            if terminal_state(state, goal_inc=True):\n",
    "                policy[state['y'],state['x']] = 'X'\n",
    "                continue\n",
    "\n",
    "            # Get the best action\n",
    "            best_r_action = -np.inf\n",
    "            for action, _, arrow in actions:\n",
    "                state_a = state.copy()\n",
    "                reward = extended_move(state_a, action, correction=False) if diag_move else move(state_a, action, correction=False)\n",
    "\n",
    "                if terminal_state(state_a, goal_inc=False):\n",
    "                    continue \n",
    "\n",
    "                a_move = V[state_a['y'], state_a['x']]\n",
    "                best_r_action = a_move if a_move > best_r_action else best_r_action\n",
    "\n",
    "            # Assign arrow(s) (it can be that multiple\n",
    "            # states are sharing the same reward)\n",
    "            for action, _, arrow in actions:\n",
    "                state_a = state.copy()\n",
    "                reward = extended_move(state_a, action) if diag_move else move(state_a, action)\n",
    "\n",
    "                if terminal_state(state_a, goal_inc=False):\n",
    "                    continue \n",
    "\n",
    "                if V[state_a['y'], state_a['x']] == best_r_action:\n",
    "                    state_policy += arrow\n",
    "\n",
    "            # Insert the arrow in the policy\n",
    "            policy[state['y'],state['x']] = state_policy\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "#### Expected value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.375,   3.625,  -3.25 ,  -3.25 ,  -4.625,  -7.   ,   2.25 ,\n",
       "         -0.125, -20.   ],\n",
       "       [ -3.25 ,  -2.   ,  -2.   ,  -3.375, -15.25 ,   0.   , -15.25 ,\n",
       "          0.   , -20.125],\n",
       "       [-16.5  ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   , -17.625,\n",
       "          0.   ,  -7.5  ],\n",
       "       [-16.5  ,   0.   ,   2.5  ,   2.5  ,   2.5  , -16.25 ,   0.   ,\n",
       "         57.375,   0.   ],\n",
       "       [ -4.625,   3.5  , -11.5  ,   0.   ,   0.   ,   0.   ,  -5.75 ,\n",
       "         -3.375,  -6.5  ],\n",
       "       [ -4.625,  -3.375,  -2.   ,  -5.75 ,  -5.75 ,  -5.75 , -15.25 ,\n",
       "          0.   , -21.5  ],\n",
       "       [-16.5  ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,\n",
       "         -5.75 , -21.5  ],\n",
       "       [  2.25 ,   4.875,   6.25 ,   6.25 ,   2.5  ,   6.25 ,  -0.625,\n",
       "        -11.5  ,   0.   ],\n",
       "       [ -1.375,   3.625,   5.   , -13.75 ,   0.   ,   1.25 ,  -1.875,\n",
       "        -15.125,   0.   ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_state_value_function(actions, iter_lim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2\n",
    "\n",
    "#### Optimal value $V^*(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -55.72233031,  -60.28711964,  -76.2187774 ,  -85.94227143,\n",
       "         -95.17265171, -102.06584518, -109.1497957 , -128.03694587,\n",
       "        -154.35820998],\n",
       "       [ -70.23910801,  -71.03512149,  -82.42332396,  -97.04567539,\n",
       "        -113.44958832,    0.        , -138.84246494,    0.        ,\n",
       "        -140.12747124],\n",
       "       [-114.35530129,    0.        ,    0.        ,    0.        ,\n",
       "           0.        ,    0.        , -156.42672419,    0.        ,\n",
       "         -71.57989163],\n",
       "       [-113.49035193,    0.        ,  -67.70838784,  -82.25522086,\n",
       "        -104.2432133 , -131.62855066,    0.        ,   67.97547745,\n",
       "           0.        ],\n",
       "       [ -66.87419882,  -61.7022072 ,  -77.43326267,    0.        ,\n",
       "           0.        ,    0.        ,  -51.56820765,  -41.72566933,\n",
       "         -63.11667048],\n",
       "       [ -64.48159917,  -63.86372895,  -69.52992994,  -77.89104824,\n",
       "         -83.19067425,  -88.81991069,  -95.51570805,    0.        ,\n",
       "        -145.95801304],\n",
       "       [ -81.40779974,    0.        ,    0.        ,    0.        ,\n",
       "           0.        ,    0.        ,    0.        , -146.67829987,\n",
       "        -167.44391259],\n",
       "       [ -24.06403261,  -19.82047396,  -28.49113241,  -42.30294539,\n",
       "         -57.38418682,  -75.05396632, -100.94960102, -122.14296902,\n",
       "           0.        ],\n",
       "       [ -29.37020784,  -33.21728211,  -49.02465928,  -73.9060024 ,\n",
       "           0.        ,  -85.55182017, -105.09043508, -125.18325993,\n",
       "           0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_task31 = compute_state_value_function(actions)\n",
    "result_task31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative difference between our result and the \"ground truth\" result is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34249411007374775"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(abs(solution_task31 - result_task31).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal policy $\\pi^*(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['→', '←', '←', '←', '←', '←', '←', '←', '←'],\n",
       "       ['↑', '↑', '←', '←', '↑', 'X', '↑', 'X', '↓'],\n",
       "       ['↑', 'X', 'X', 'X', 'X', 'X', '↑', 'X', '↓'],\n",
       "       ['↓', 'X', '↓', '←', '←', '←', 'X', '→', '@'],\n",
       "       ['→', '↓', '←', 'X', 'X', 'X', '→', '↑', '↑'],\n",
       "       ['→', '↑', '←', '←', '←', '←', '↑', 'X', '↑'],\n",
       "       ['↓', 'X', 'X', 'X', 'X', 'X', 'X', '↓', '↑'],\n",
       "       ['→', '←', '←', '←', '←', '←', '←', '←', 'X'],\n",
       "       ['↑', '↑', '↑', '↑', 'X', '↑', '←', '←', 'X']], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(result_task31, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3\n",
    "Actions extended allowing diagonal moves, such that the agent can move to its eight neighboring cells.\n",
    "\n",
    "#### Optimal value $V^*(s)$\n",
    "The Policy iteration algoritm changed the value state function table (compared to the task 3.2) by decreasing the value of almost every cell, and this is because the agent has now the possibility to explore the environment with more moves, which ultimately increases the number of time it incours in negative rewards due to how the environment is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -78.85295654,  -84.78663318, -100.82983448, -111.41432921,\n",
       "        -122.39915622, -132.93539283, -144.82161683, -170.07232622,\n",
       "        -207.24509049],\n",
       "       [ -87.24992879,  -88.45634884,  -98.81538951, -113.13610521,\n",
       "        -129.96200609,    0.        , -140.33734441,    0.        ,\n",
       "        -179.61116839],\n",
       "       [-131.36156914,    0.        ,    0.        ,    0.        ,\n",
       "           0.        ,    0.        , -104.20498146,    0.        ,\n",
       "         -90.89994106],\n",
       "       [-126.45709436,    0.        ,  -67.15456507,  -77.81474247,\n",
       "         -94.73275553, -112.8608242 ,    0.        ,   35.28098586,\n",
       "           0.        ],\n",
       "       [ -76.35713138,  -70.98274204,  -85.70573896,    0.        ,\n",
       "           0.        ,    0.        ,  -60.32823267,  -59.43878516,\n",
       "         -78.48338783],\n",
       "       [ -76.1775224 ,  -75.05042911,  -81.76856604,  -89.43287221,\n",
       "         -96.023486  ,  -99.67864854, -110.44750388,    0.        ,\n",
       "        -148.21199079],\n",
       "       [ -91.26110919,    0.        ,    0.        ,    0.        ,\n",
       "           0.        ,    0.        ,    0.        , -147.62609185,\n",
       "        -177.50236181],\n",
       "       [ -41.96712871,  -41.94574064,  -49.16841751,  -62.22486027,\n",
       "         -77.79790519,  -96.17140595, -118.73061694, -137.8550347 ,\n",
       "           0.        ],\n",
       "       [ -46.20824226,  -49.99540291,  -64.40336129,  -88.10520488,\n",
       "           0.        , -100.54463637, -122.6426868 , -142.37754211,\n",
       "           0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_task33=compute_state_value_function(actions_diag, diag_move=True)\n",
    "result_task33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal policy $\\pi^*(s)$\n",
    "The current policy is more flexible and successful than the policy in task 3.2 because now the agent is allowed to navigate routes that were previously forbidden and because the number of steps required to reach the destination is now reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['→', '←', '←', '↙', '←', '←', '←', '↙', '←'],\n",
       "       ['↑', '↖', '↖', '←', '↖', 'X', '↓', 'X', '↓'],\n",
       "       ['↑', 'X', 'X', 'X', 'X', 'X', '↘', 'X', '↓'],\n",
       "       ['↘', 'X', '↙', '←', '←', '↘', 'X', '→', '@'],\n",
       "       ['→', '↗', '↑', 'X', 'X', 'X', '↗', '↗', '↑'],\n",
       "       ['↗', '↑', '↖', '←', '←', '↗', '↗', 'X', '↖'],\n",
       "       ['↘', 'X', 'X', 'X', 'X', 'X', 'X', '↖', '↙'],\n",
       "       ['→', '←', '←', '←', '←', '←', '←', '←', 'X'],\n",
       "       ['↗', '↑', '↖', '↖', 'X', '↖', '↖', '↖', 'X']], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(result_task33, actions_diag, diag_move=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.4\n",
    "\n",
    "#### Optimal value $V^*(s)$\n",
    "The Policy iteration algoritm changed the value state function table (compared to the task 3.3) by increasing the value of the cells around the goal cell and by decreasing the value of those around the border."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -86.03140441,  -89.9602198 , -104.64938837, -114.89711383,\n",
       "        -125.30481492, -133.83940365, -145.40422954, -168.42400583,\n",
       "        -206.9191005 ],\n",
       "       [ -88.74772565,  -89.94085533, -100.48055425, -113.46949619,\n",
       "        -129.02650581,    0.        , -136.31702458,    0.        ,\n",
       "        -180.02396947],\n",
       "       [-120.10731729,    0.        ,    0.        ,    0.        ,\n",
       "           0.        ,    0.        ,  -82.11356971,    0.        ,\n",
       "         -94.84346552],\n",
       "       [-110.72060201,    0.        ,  -62.83646447,  -68.45619406,\n",
       "         -78.82369046,  -88.95581937,    0.        ,   16.85273294,\n",
       "           0.        ],\n",
       "       [ -72.72754865,  -64.51986164,  -76.87092169,    0.        ,\n",
       "           0.        ,    0.        ,  -52.5131928 ,  -54.17135432,\n",
       "         -80.57906673],\n",
       "       [ -73.51265094,  -72.09177902,  -80.60962022,  -87.08490005,\n",
       "         -90.68742038,  -88.88222659,  -99.89407572,    0.        ,\n",
       "        -147.1593049 ],\n",
       "       [ -79.29326108,    0.        ,    0.        ,    0.        ,\n",
       "           0.        ,    0.        ,    0.        , -144.88837165,\n",
       "        -177.32636569],\n",
       "       [ -47.6569712 ,  -47.48749513,  -55.30110605,  -67.68235615,\n",
       "         -82.08669175,  -99.94340891, -121.7077271 , -140.79831468,\n",
       "           0.        ],\n",
       "       [ -51.78590855,  -53.38058215,  -65.43322382,  -87.24233222,\n",
       "           0.        , -104.80976611, -126.31507178, -146.24761507,\n",
       "           0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_task34 = compute_state_value_function(actions_diag, diag_move=True, deterministic=False)\n",
    "result_task34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal policy $\\pi^*(s)$\n",
    "The current policy didn't changed significaly compared to the policy of task 3.3, but as it possible to notice in the cell x:6, y:5 (starting to count from 0) the arrow changed from '↗', to '↑' increasing by one the number of required steps to reach the destina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['↓', '←', '↙', '↙', '↙', '←', '←', '↙', '←'],\n",
       "       ['↑', '↖', '←', '←', '←', 'X', '↓', 'X', '↓'],\n",
       "       ['↑', 'X', 'X', 'X', 'X', 'X', '↘', 'X', '↓'],\n",
       "       ['↘', 'X', '↙', '←', '←', '↘', 'X', '→', '@'],\n",
       "       ['→', '↗', '↑', 'X', 'X', 'X', '↗', '↗', '↑'],\n",
       "       ['↗', '↑', '↖', '↖', '←', '↗', '↑', 'X', '↖'],\n",
       "       ['↘', 'X', 'X', 'X', 'X', 'X', 'X', '↖', '↙'],\n",
       "       ['→', '←', '←', '←', '←', '←', '←', '←', 'X'],\n",
       "       ['↗', '↑', '↖', '↖', 'X', '↖', '↖', '↖', 'X']], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(result_task34, actions_diag, diag_move=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['→', '←', '←', '↙', '←', '←', '←', '↙', '←'],\n",
       "       ['↑', '↖', '↖', '←', '↖', 'X', '↓', 'X', '↓'],\n",
       "       ['↑', 'X', 'X', 'X', 'X', 'X', '↘', 'X', '↓'],\n",
       "       ['↘', 'X', '↙', '←', '←', '↘', 'X', '→', '@'],\n",
       "       ['→', '↗', '↑', 'X', 'X', 'X', '↗', '↗', '↑'],\n",
       "       ['↗', '↑', '↖', '←', '←', '↗', '↗', 'X', '↖'],\n",
       "       ['↘', 'X', 'X', 'X', 'X', 'X', 'X', '↖', '↙'],\n",
       "       ['→', '←', '←', '←', '←', '←', '←', '←', 'X'],\n",
       "       ['↗', '↑', '↖', '↖', 'X', '↖', '↖', '↖', 'X']], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(result_task33, actions_diag, diag_move=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

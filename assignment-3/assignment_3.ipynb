{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Print options\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "#Â Sanity check data\n",
    "grid_world = np.load('data/gridworld.npy')\n",
    "solution_task31 = np.load('data/stateValuesTask1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "DISCOUNT = 0.9\n",
    "THRESHOLD = 0.007\n",
    "GOAL_S = {'x':8,'y':3}\n",
    "actions = [('r', 0.625, u'\\u2192'), ('l', 0.125, u'\\u2190'), ('u', 0.125, u'\\u2191'), ('d', 0.125, u'\\u2193')]\n",
    "actions_diag = [('r', 0.5625, u'\\u2192'), ('l', 0.0625, u'\\u2190'), ('u', 0.0625, u'\\u2191'), ('d', 0.0625, u'\\u2193'), \n",
    "                ('lu', 0.0625, u'\\u2196'), ('ru', 0.0625, u'\\u2197'), ('rd', 0.0625, u'\\u2198'), ('ld', 0.0625, u'\\u2199')]\n",
    "\n",
    "# Common routines\n",
    "def is_outside(state):\n",
    "    return ((state['y'] < 0 or state['y'] >= grid_world.shape[0]) \n",
    "              or (state['x'] < 0 or state['x'] >= grid_world.shape[1]))\n",
    "\n",
    "def terminal_state(state, goal_inc=True):\n",
    "    # Entered a goal state\n",
    "    if goal_inc and state == GOAL_S:\n",
    "        return True\n",
    "    # Exeeded boundary limits\n",
    "    elif is_outside(state):\n",
    "        return True\n",
    "    # Entered a cell X\n",
    "    elif grid_world[state['y'], state['x']] == -20:\n",
    "        return True\n",
    "\n",
    "    # Not a terminal state\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_reward(state):\n",
    "    if is_outside(state):\n",
    "        return -30;\n",
    "    else:\n",
    "        return grid_world[state['y'], state['x']]\n",
    "\n",
    "def move(state, action, correction=True):\n",
    "    state_c = state.copy()\n",
    "    \n",
    "    if action == 'l':\n",
    "        state_c['x']  -= 1\n",
    "    elif action == 'r':\n",
    "        state_c['x']  += 1\n",
    "    elif action == 'u':\n",
    "        state_c['y']  -= 1\n",
    "    elif action == 'd':\n",
    "        state_c['y']  += 1\n",
    "        \n",
    "    if not terminal_state(state_c, goal_inc=False) or not correction:\n",
    "        state['x'] = state_c['x']\n",
    "        state['y'] = state_c['y']\n",
    "        \n",
    "    return get_reward(state_c)\n",
    "\n",
    "def extended_move(state, action, correction=True):\n",
    "    state_c = state.copy()\n",
    "    \n",
    "    if action == 'l':\n",
    "        state_c['x'] -= 1\n",
    "    elif action == 'r':\n",
    "        state_c['x'] += 1\n",
    "    elif action == 'u':\n",
    "        state_c['y'] -= 1\n",
    "    elif action == 'd':\n",
    "        state_c['y'] += 1\n",
    "    elif action == 'lu':\n",
    "        state_c['x'] -= 1\n",
    "        state_c['y'] -= 1\n",
    "    elif action == 'ru':\n",
    "        state_c['x'] += 1\n",
    "        state_c['y'] -= 1\n",
    "    elif action == 'ld':\n",
    "        state_c['x'] -= 1\n",
    "        state_c['y'] += 1\n",
    "    elif action == 'rd':\n",
    "        state_c['x'] += 1\n",
    "        state_c['y'] += 1\n",
    "        \n",
    "    if not terminal_state(state_c, goal_inc=False) or not correction:\n",
    "        state['x'] = state_c['x']\n",
    "        state['y'] = state_c['y']\n",
    "        \n",
    "    return get_reward(state_c)\n",
    "\n",
    "def get_actions(action):\n",
    "    if action == 'l':\n",
    "        return ['l', 'lu', 'ld']\n",
    "    if action == 'r':\n",
    "        return ['r', 'ru', 'rd']\n",
    "    if action == 'u':\n",
    "        return ['u', 'ru', 'lu']\n",
    "    if action == 'd':\n",
    "        return ['d', 'dl', 'dr']\n",
    "    if action == 'lu':\n",
    "        return ['lu', 'l', 'u']\n",
    "    if action == 'ld':\n",
    "        return ['ld', 'l', 'd']\n",
    "    if action == 'ru':\n",
    "        return ['ru', 'r', 'u']\n",
    "    if action == 'rd':\n",
    "        return ['rd', 'r', 'd']\n",
    "\n",
    "def non_deterministic_move(state, action, V):    \n",
    "    # Transitions probabilities\n",
    "    transitions = [0.8, 0.1, 0.1]\n",
    "    \n",
    "    # Rewards\n",
    "    rewards = []\n",
    "    \n",
    "    # States values\n",
    "    state_values = []\n",
    "    \n",
    "    # Get possible actions\n",
    "    actions = get_actions(action)\n",
    "    \n",
    "    # Populate actions holders\n",
    "    for action_ in actions:\n",
    "        # Copy of state\n",
    "        state_a = state.copy()\n",
    "        \n",
    "        # Populate lists\n",
    "        rewards.append(extended_move(state_a, action_))\n",
    "        state_values.append(V[state_a['y'], state_a['x']])\n",
    "    \n",
    "    return (transitions, rewards, state_values)\n",
    "    \n",
    "def compute_state_value_function(actions, diag_move=False, deterministic=True):\n",
    "    # Value function matrix\n",
    "    V = np.zeros_like(grid_world)\n",
    "    \n",
    "    # Delta value\n",
    "    delta = np.inf\n",
    "    \n",
    "    while delta > THRESHOLD:\n",
    "        # Reset delta\n",
    "        delta  = 0\n",
    "\n",
    "        # Copy of V\n",
    "        V_copy = V.copy()\n",
    "        \n",
    "        # States loop\n",
    "        for y in range(grid_world.shape[0]):\n",
    "            for x in range(grid_world.shape[1]):\n",
    "                # Current state\n",
    "                state = {'x':x,'y':y}\n",
    "                \n",
    "                if terminal_state(state):\n",
    "                    continue\n",
    "        \n",
    "                # Temporary value for the state\n",
    "                V_prev = V[y,x]\n",
    "                \n",
    "                # Reset value\n",
    "                V[y,x] = 0\n",
    "                \n",
    "                # Compute new value (diag move allowed)\n",
    "                for action, probability, _ in actions:\n",
    "                    state_a = state.copy()\n",
    "                    \n",
    "                    if (not deterministic):\n",
    "                        trans, rewards, state_values = non_deterministic_move(state_a, action, V_copy)\n",
    "                        for x in range(3):\n",
    "                            V[y,x] += probability * (trans[x] * (rewards[x] + DISCOUNT * state_values[x]))\n",
    "                    else:\n",
    "                        reward = extended_move(state_a, action) if diag_move else move(state_a, action)\n",
    "                        V[y,x] += probability * (reward + DISCOUNT * V_copy[state_a['y'], state_a['x']])\n",
    "                \n",
    "                # Compute new delta\n",
    "                delta = max(delta, np.abs(V_prev - V[y,x]))\n",
    "    return V\n",
    "\n",
    "def compute_action_value_function(state_value_table, actions, diag_move=False):\n",
    "    # State-Value and Action-Value\n",
    "    # tables to be used and computed\n",
    "    V = state_value_table.copy()\n",
    "    policy = V.copy().astype(object)\n",
    "\n",
    "    # Set goal cell value to inf\n",
    "    V[GOAL_S['y'], GOAL_S['x']] = np.inf\n",
    "\n",
    "    # Loop over every state\n",
    "    for y in range(grid_world.shape[0]):\n",
    "        for x in range(grid_world.shape[1]):\n",
    "            # Current state\n",
    "            state = {'x':x,'y':y}\n",
    "            \n",
    "            # State policy\n",
    "            state_policy = ''\n",
    "\n",
    "            # Check if state is Goal\n",
    "            if state == GOAL_S:\n",
    "                policy[state['y'],state['x']] = '@'\n",
    "                continue\n",
    "                \n",
    "            # Check if terminal state\n",
    "            if terminal_state(state, goal_inc=True):\n",
    "                policy[state['y'],state['x']] = 'X'\n",
    "                continue\n",
    "\n",
    "            # Get the best action\n",
    "            best_r_action = -np.inf\n",
    "            for action, _, arrow in actions:\n",
    "                state_a = state.copy()\n",
    "                reward = extended_move(state_a, action, correction=False) if diag_move else move(state_a, action, correction=False)\n",
    "\n",
    "                if terminal_state(state_a, goal_inc=False):\n",
    "                    continue \n",
    "\n",
    "                a_move = V[state_a['y'], state_a['x']]\n",
    "                best_r_action = a_move if a_move > best_r_action else best_r_action\n",
    "\n",
    "            # Assign arrow(s) (it can be that multiple\n",
    "            # states are sharing the same reward)\n",
    "            for action, _, arrow in actions:\n",
    "                state_a = state.copy()\n",
    "                reward = extended_move(state_a, action) if diag_move else move(state_a, action)\n",
    "\n",
    "                if terminal_state(state_a, goal_inc=False):\n",
    "                    continue \n",
    "\n",
    "                if V[state_a['y'], state_a['x']] == best_r_action:\n",
    "                    state_policy += arrow\n",
    "\n",
    "            # Insert the arrow in the policy\n",
    "            policy[state['y'],state['x']] = state_policy\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulative difference from the solution results: 0.34249411007374775\n",
      "\n",
      "[[ -55.72233031  -60.28711964  -76.2187774   -85.94227143  -95.17265171\n",
      "  -102.06584518 -109.1497957  -128.03694587 -154.35820998]\n",
      " [ -70.23910801  -71.03512149  -82.42332396  -97.04567539 -113.44958832\n",
      "     0.         -138.84246494    0.         -140.12747124]\n",
      " [-114.35530129    0.            0.            0.            0.\n",
      "     0.         -156.42672419    0.          -71.57989163]\n",
      " [-113.49035193    0.          -67.70838784  -82.25522086 -104.2432133\n",
      "  -131.62855066    0.           67.97547745    0.        ]\n",
      " [ -66.87419882  -61.7022072   -77.43326267    0.            0.\n",
      "     0.          -51.56820765  -41.72566933  -63.11667048]\n",
      " [ -64.48159917  -63.86372895  -69.52992994  -77.89104824  -83.19067425\n",
      "   -88.81991069  -95.51570805    0.         -145.95801304]\n",
      " [ -81.40779974    0.            0.            0.            0.\n",
      "     0.            0.         -146.67829987 -167.44391259]\n",
      " [ -24.06403261  -19.82047396  -28.49113241  -42.30294539  -57.38418682\n",
      "   -75.05396632 -100.94960102 -122.14296902    0.        ]\n",
      " [ -29.37020784  -33.21728211  -49.02465928  -73.9060024     0.\n",
      "   -85.55182017 -105.09043508 -125.18325993    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "result_task31 = compute_state_value_function(actions)\n",
    "diff = sum(abs(solution_task31 - result_task31).flatten())\n",
    "\n",
    "print(\"Accumulative difference from the solution results: {}\\n\\n{}\".format(diff, result_task31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy32 = compute_action_value_function(result_task31, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3.2 policy: \n",
      "\n",
      " [['â' 'â' 'â' 'â' 'â' 'â' 'â' 'â' 'â']\n",
      " ['â' 'â' 'â' 'â' 'â' 'X' 'â' 'X' 'â']\n",
      " ['â' 'X' 'X' 'X' 'X' 'X' 'â' 'X' 'â']\n",
      " ['â' 'X' 'â' 'â' 'â' 'â' 'X' 'â' '@']\n",
      " ['â' 'â' 'â' 'X' 'X' 'X' 'â' 'â' 'â']\n",
      " ['â' 'â' 'â' 'â' 'â' 'â' 'â' 'X' 'â']\n",
      " ['â' 'X' 'X' 'X' 'X' 'X' 'X' 'â' 'â']\n",
      " ['â' 'â' 'â' 'â' 'â' 'â' 'â' 'â' 'X']\n",
      " ['â' 'â' 'â' 'â' 'X' 'â' 'â' 'â' 'X']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Task 3.2 policy: \\n\\n\", policy32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulative difference from the solution 3.2 results: 1024.1516773911771\n",
      "\n",
      "[[ -78.85295654  -84.78663318 -100.82983448 -111.41432921 -122.39915622\n",
      "  -132.93539283 -144.82161683 -170.07232622 -207.24509049]\n",
      " [ -87.24992879  -88.45634884  -98.81538951 -113.13610521 -129.96200609\n",
      "     0.         -140.33734441    0.         -179.61116839]\n",
      " [-131.36156914    0.            0.            0.            0.\n",
      "     0.         -104.20498146    0.          -90.89994106]\n",
      " [-126.45709436    0.          -67.15456507  -77.81474247  -94.73275553\n",
      "  -112.8608242     0.           35.28098586    0.        ]\n",
      " [ -76.35713138  -70.98274204  -85.70573896    0.            0.\n",
      "     0.          -60.32823267  -59.43878516  -78.48338783]\n",
      " [ -76.1775224   -75.05042911  -81.76856604  -89.43287221  -96.023486\n",
      "   -99.67864854 -110.44750388    0.         -148.21199079]\n",
      " [ -91.26110919    0.            0.            0.            0.\n",
      "     0.            0.         -147.62609185 -177.50236181]\n",
      " [ -41.96712871  -41.94574064  -49.16841751  -62.22486027  -77.79790519\n",
      "   -96.17140595 -118.73061694 -137.8550347     0.        ]\n",
      " [ -46.20824226  -49.99540291  -64.40336129  -88.10520488    0.\n",
      "  -100.54463637 -122.6426868  -142.37754211    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "result_task33 = compute_state_value_function(actions_diag, diag_move=True)\n",
    "diff = sum(abs(result_task33 - result_task31).flatten())\n",
    "\n",
    "print(\"Accumulative difference from the solution 3.2 results: {}\\n\\n{}\".format(diff, result_task33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy33 = compute_action_value_function(result_task33, actions_diag, diag_move=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3.3 policy: \n",
      "\n",
      " [['â' 'â' 'â' 'â' 'â' 'â' 'â' 'â' 'â']\n",
      " ['â' 'â' 'â' 'â' 'â' 'X' 'â' 'X' 'â']\n",
      " ['â' 'X' 'X' 'X' 'X' 'X' 'â' 'X' 'â']\n",
      " ['â' 'X' 'â' 'â' 'â' 'â' 'X' 'â' '@']\n",
      " ['â' 'â' 'â' 'X' 'X' 'X' 'â' 'â' 'â']\n",
      " ['â' 'â' 'â' 'â' 'â' 'â' 'â' 'X' 'â']\n",
      " ['â' 'X' 'X' 'X' 'X' 'X' 'X' 'â' 'â']\n",
      " ['â' 'â' 'â' 'â' 'â' 'â' 'â' 'â' 'X']\n",
      " ['â' 'â' 'â' 'â' 'X' 'â' 'â' 'â' 'X']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Task 3.3 policy: \\n\\n\", policy33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_task34 = compute_state_value_function(actions_diag, diag_move=True, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
